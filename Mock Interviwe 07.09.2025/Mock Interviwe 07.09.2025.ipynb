{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143f59c7",
   "metadata": {},
   "source": [
    "# Mock interview Seite f√ºr Studenten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaba01a",
   "metadata": {},
   "source": [
    "## F√ºr dieses Interview wirst du mit dem folgenden Datensatz arbeiten. Schau ihn dir gerne in Ruhe an, um dich damit vertraut zu machen. \n",
    "- loans_modified.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb16c9c",
   "metadata": {},
   "source": [
    " - Akzeptanz/Ablehnung (Scoring): Soll der Kreditantrag angenommen oder abgelehnt werden? (Dies wird oft durch ein Kredit-Scoring-Modell gel√∂st).\n",
    "\n",
    "        - Kreditnehmer-Merkmale:\n",
    "\n",
    "        - Demografische Daten (Alter, Wohnort, Familienstand).\n",
    "\n",
    "        - Finanzielle Daten (Einkommen, Besch√§ftigungsstatus, Schulden, Verm√∂gen).\n",
    "\n",
    "        - Kredit-Historie (Anzahl der Kredite, bisherige Zahlungsmoral, Schufa-Score).\n",
    "        \n",
    " - Risikopreisgestaltung: Welcher Zinssatz sollte dem Kunden angeboten werden, um das damit verbundene Ausfallrisiko angemessen zu bepreisen?\n",
    "\n",
    "        - Kredith√∂he (Loan Amount).\n",
    "\n",
    "        - Laufzeit (Term).\n",
    "\n",
    "        - Verwendungszweck (Purpose).\n",
    "\n",
    " - Kapitalbedarf (Regulatorik): Wie viel Eigenkapital muss die Bank gem√§√ü regulatorischen Vorgaben (z.B. Basel III) f√ºr diesen Kredit zur√ºcklegen?\n",
    "\n",
    "        - Die wichtigste Information: Ist der Kredit ausgefallen (z.B. mehr als 90 Tage √ºberf√§llig) oder wurde er ordnungsgem√§√ü zur√ºckgezahlt?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49428f86",
   "metadata": {},
   "source": [
    " ## Ziel des Interviews:\n",
    " - Erstelle ein Modell zur Analyse von Darlehen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbfdb8",
   "metadata": {},
   "source": [
    "# Schritte zur Vorbereitung:\n",
    "- 1. Mach dich mit den Daten vertraut.\n",
    "- 2. Versuch, ein Modell zu erstellen.\n",
    "- 3. Wir empfehlen dir, ein Notebook bereitzuhalten, um w√§hrend des Interviews Notizen zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Laden von CSV-Dateien in Colab/VSCode-Umgebung\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from tkinter import filedialog,Tk\n",
    "import numpy as np\n",
    "from typing import Dict, List, Callable, Union \n",
    "from pandas.api.types import is_numeric_dtype \n",
    "\n",
    "# Einstellungen\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Erkennung der Umgebung (Google Colab oder lokal)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    COLAB_ENV = True\n",
    "except ImportError:\n",
    "    COLAB_ENV = False\n",
    "\n",
    "# ====== DATEI LADEN ======\n",
    "print('*' * 10, 'DATEI LADEN', '*' * 10)\n",
    "\n",
    "try:\n",
    "    if COLAB_ENV:\n",
    "        # Code f√ºr Google Colab\n",
    "        print(\"Google Colab Umgebung erkannt. Bitte Datei hochladen.\")\n",
    "        uploaded = files.upload()\n",
    "        file_name = list(uploaded.keys())[0]\n",
    "        print(f\"\\nDie Datei '{file_name}' wurde erfolgreich hochgeladen und steht nun zur Verf√ºgung.\")\n",
    "        df = pd.read_csv(file_name)\n",
    "    else:\n",
    "        # Code f√ºr lokale Umgebung mit Tkinter-Dateiauswahlfenster\n",
    "        print(\"Lokale Umgebung erkannt. Es √∂ffnet sich ein Dateiauswahlfenster.\")\n",
    "        \n",
    "        # Erstellen eines Tkinter-Root-Fensters und Ausblenden\n",
    "        root = Tk()\n",
    "        root.withdraw() \n",
    "        \n",
    "        # √ñffnen des Dateidialogs und Abrufen des Pfades\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            title=\"W√§hlen Sie Ihre CSV-Datei\",\n",
    "            filetypes=((\"CSV-Dateien\", \"*.csv\"), (\"Alle Dateien\", \"*.*\"))\n",
    "        )\n",
    "        \n",
    "        if file_path:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"\\nDie Datei '{file_path}' wurde erfolgreich geladen.\")\n",
    "        else:\n",
    "            print(\"Keine Datei ausgew√§hlt.\")\n",
    "            df = None\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d986d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è DataFrames als CSV-Datei in df benennen\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "print('*' * 10, 'DATEN LADEN', '*' * 10)\n",
    "\n",
    "df = pd.read_csv('loans_modified.csv')\n",
    "\n",
    "# Erfolgsnachweis: Ausgabe der Dateninformationen\n",
    "print(f\"Daten erfolgreich geladen! DataFrame-Gr√∂√üe: {df.shape}\")\n",
    "print(\"\\nErste 5 Zeilen des geladenen DataFrames zur √úberpr√ºfung:\")\n",
    "print(df.head().to_string())\n",
    "\n",
    "print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b367f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Analyse der Daten Type / Unique(count)/ Duplicate (count) / NaN / Ausreisser \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from typing import Dict, List, Callable, Union\n",
    "\n",
    "# Zuweisung, wie vom Benutzer gew√ºnscht\n",
    "df_check = df\n",
    "\n",
    "def generate_cleaning_muster(column: str, semantic_type: str) -> str:\n",
    "    # Nur ein Platzhalter, da keine Code-Ausgabe gew√ºnscht ist\n",
    "    return \"\"\n",
    "\n",
    "def analyze_semantic_type_v3(df_check: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analysiert die semantischen Datentypen der Spalten in einem DataFrame.\n",
    "    Die interne Logik zur Typ-Erkennung wurde von Ihrem bereitgestellten Muster √ºbernommen.\n",
    "    \"\"\"\n",
    "    SEMANTIC_HINTS_PRIORITY: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
    "        'ID': {\n",
    "            'keywords': {'_id', 'session_id', 'trip_id', 'user_id', 'unique_id', 'kundennummer', 'bestellnr', 'order_id', 'artikelnummer'},\n",
    "            'validation_func': lambda series: ((series.dropna().astype(str).apply(len) >= 5).any())\n",
    "        },\n",
    "        'Datum/Zeit': {\n",
    "            'keywords': {'datum', 'zeit', 'date', 'time', 'start', 'end', 'birthdate', 'signup_date', 'check_in', 'check_out', 'departure', 'return', 'geburtstag', 'timestamp', 'creation_date', 'modified_date', 'erstellt'},\n",
    "            'validation_func':lambda series: ((series.dropna().nunique() == 12 or series.dropna().nunique() == 31) or (series.dropna().apply(lambda x: isinstance(x, str)).all() and (pd.to_datetime(series.dropna(), errors='coerce').notna().all() or (series.dropna().astype(str).str.contains(r'[-_/]', na=False).any() and series.dropna().astype(str).str.contains(r'\\d{4}', na=False).any()))))\n",
    "        },\n",
    "        'Geometrisch': {\n",
    "            'keywords': {'geom', 'geometry', 'shape', 'wkt', 'geojson', 'coordinates', 'location_data'},\n",
    "            'validation_func': lambda series: (series.dropna().astype(str).str.contains(r'^(POINT|LINESTRING|POLYGON|MULTIPOINT|MULTILINESTRING|MULTIPOLYGON)\\s*\\(', regex=True, na=False).any() or series.dropna().astype(str).str.contains(r'{\"type\":\\s*\"(Point|LineString|Polygon|MultiPoint|MultiLineString|MultiPolygon)\"', regex=True, na=False).any())\n",
    "        },\n",
    "    }\n",
    "    SEMANTIC_HINTS_TEXT: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
    "        'Text (Kategorisch)': {\n",
    "            'keywords': {'city', 'country', 'l√§nder', 'region', 'state', 'bundesland', 'zip', 'plz'},\n",
    "            'validation_func': lambda series: series.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(series.dropna()) or isinstance(series.dropna().dtype, pd.CategoricalDtype))\n",
    "        },\n",
    "         'Text (Gender)': {\n",
    "            'keywords': {'geschlecht', 'typ', 'category', 'art', 'gender'},\n",
    "            'validation_func': lambda series: series.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(series.dropna()) or isinstance(series.dropna().dtype, pd.CategoricalDtype))\n",
    "        },\n",
    "        'Text (object)': {\n",
    "            'keywords': {'airport', 'destination', 'origin', 'heimat', 'status'},\n",
    "            'validation_func': lambda series: series.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(series.dropna()) or isinstance(series.dropna().dtype, pd.CategoricalDtype))\n",
    "        },\n",
    "        'Text (Freitext)': {\n",
    "            'keywords': {'name', 'hotel', 'airline', 'beschreibung', 'kommentar', 'nachricht', 'adresse'},\n",
    "            'validation_func': lambda series: pd.api.types.is_string_dtype(series.dropna()) or isinstance(series.dropna().dtype, pd.CategoricalDtype)\n",
    "        },\n",
    "    }\n",
    "    SEMANTIC_HINTS_NUMERIC: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
    "        'Boolean(NaN)': {\n",
    "            'keywords': {'is_invalid','missing', 'is_missing', 'has_value', 'exists', 'is_null', 'is_na', 'isnan', 'filled','is_outlier'},\n",
    "            'validation_func': lambda series: (series.dropna().nunique() >= 1) and (pd.api.types.is_bool_dtype(series.dropna()) or set(series.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
    "        },\n",
    "        'Boolean': {\n",
    "            'keywords': {'self_employed','is_weekend_trip', 'boolean', 'bool', 'booked', 'married', 'cancellation', 'children','discount', 'flight_booked', 'hotel_booked', 'return_flight_booked', 'is_cancelled'},\n",
    "            'validation_func': lambda series: (series.dropna().nunique() == 2) and (pd.api.types.is_bool_dtype(series.dropna()) or set(series.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
    "        },\n",
    "        'Float (Geografisch)': {\n",
    "            'keywords': {'lat', 'lon', 'latitude', 'longitude'},\n",
    "            'validation_func': lambda series: pd.to_numeric(series.dropna(), errors='coerce').notna().all() and (pd.to_numeric(series.dropna(), errors='coerce').astype(str).str.count(r'\\.').all() or pd.api.types.is_float_dtype(series.dropna()))\n",
    "        },\n",
    "        'Float (Prozentsatz)': {\n",
    "            'keywords': {'percent', 'pct', 'rate', 'discount', '%'},\n",
    "            'validation_func': lambda series: (series.dropna().nunique() > 2) and ((pd.to_numeric(series.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 1).all() or pd.to_numeric(series.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 100).all()) or (pd.to_numeric(series.dropna().astype(str).str.replace('%', ''), errors='coerce').notna().all() and series.dropna().astype(str).str.replace('%', '').str.replace(',', '.').str.match(r'^\\d{1,3}(\\.\\d{1,3})?$').all()))\n",
    "        },\n",
    "        'Float (Waehrung)': {\n",
    "            'keywords': {'preis', 'kosten', 'betrag', 'dollar', 'euro', 'yen', 'usd', 'eur', 'fare','chf', 'gbp', 'sek', 'jpy', '‚Ç¨', '¬£', '$'},\n",
    "            'validation_func': lambda series: (pd.api.types.is_numeric_dtype(series.dropna()) or pd.to_numeric(series.dropna().str.replace(',', '.'), errors='coerce').notna().all()) and series.dropna().nunique() > 2\n",
    "        },\n",
    "        'Integer': {\n",
    "            'keywords': {'_time_days','_duration_days','anzahl', 'menge', 'stueck', 'stk', 'count', 'qty', 'seats', 'rooms', 'nights', 'bags', 'clicks', 'nummer', 'nr', 'quantity', 'val', 'rating'},\n",
    "            'validation_func': lambda series: (series.dropna().nunique() > 2) and pd.to_numeric(series.dropna(), errors='coerce').notna().all() and (pd.to_numeric(series.dropna(), errors='coerce').dropna().apply(lambda x: x.is_integer() if isinstance(x, float) else True).all())\n",
    "        }\n",
    "    }\n",
    "    results: List[Dict[str, str]] = []\n",
    "    hint_categories = [SEMANTIC_HINTS_PRIORITY, SEMANTIC_HINTS_TEXT, SEMANTIC_HINTS_NUMERIC]\n",
    "    SEMANTIC_HINTS_NUMERIC_ORDERED: List[str] = ['Boolean(NaN)','Boolean', 'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)', 'Integer']\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "        # HIER: df muss df_check sein, um die Typen korrekt aus df_Cleaning zu ermitteln\n",
    "        # Da die Funktion analyze_semantic_type_v3 df_check als Argument erwartet, sollte df_check verwendet werden.\n",
    "        # Im Originalcode wird df verwendet, was nicht definiert ist, wenn df_check = df_Cleaning gesetzt wurde.\n",
    "        # ICH gehe davon aus, dass der Benutzer hier df_check meint, da df_check an die Funktion √ºbergeben wird.\n",
    "        df_to_analyze = df_check # Verwende den √ºbergebenen DataFrame\n",
    "\n",
    "        for column in df_to_analyze.columns:\n",
    "            original_dtype: str = str(df_to_analyze[column].dtype)\n",
    "            semantic_type: str = original_dtype\n",
    "            column_lower: str = column.lower()\n",
    "            found_match: bool = False\n",
    "\n",
    "            for hint_group in hint_categories:\n",
    "                if found_match:\n",
    "                    break\n",
    "                if hint_group is SEMANTIC_HINTS_NUMERIC:\n",
    "                    for sem_type in SEMANTIC_HINTS_NUMERIC_ORDERED:\n",
    "                        hints = hint_group[sem_type]\n",
    "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
    "                        content_valid = False\n",
    "                        try:\n",
    "                            content_valid = hints['validation_func'](df_to_analyze[column])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                        if name_match and content_valid:\n",
    "                            semantic_type = sem_type\n",
    "                            found_match = True\n",
    "                            break\n",
    "                else:\n",
    "                    for sem_type, hints in hint_group.items():\n",
    "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
    "                        content_valid = False\n",
    "                        try:\n",
    "                            content_valid = hints['validation_func'](df_to_analyze[column])\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "                        if name_match and content_valid:\n",
    "                            semantic_type = sem_type\n",
    "                            found_match = True\n",
    "                            break\n",
    "\n",
    "            results.append({\n",
    "                'Spalte': column,\n",
    "                'Semantischer Typ': semantic_type,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# HAUPT-MUSTER F√úR KONSOLIDIERTE ANZEIGE (INKL. SEMANTIK, OHNE VORSCHL√ÑGE)\n",
    "def muster_df_consolidated_view(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    F√ºhrt eine konsolidierte Datenqualit√§ts-Analyse durch, inklusive semantischer\n",
    "    Typ-Erkennung, und gibt diese in einer einzigen Tabelle aus, OHNE\n",
    "    Bereinigungsvorschl√§ge.\n",
    "    \"\"\"\n",
    "    # 1. Semantische Typ-Erkennung\n",
    "    df_sem_types = analyze_semantic_type_v3(df)\n",
    "    \n",
    "    # Dictionary zur Konsolidierung ALLER Metriken pro Spalte\n",
    "    consolidated_data: Dict[str, Dict[str, Union[str, float, int, None]]] = {}\n",
    "\n",
    "    # Initialisierung und Basis-Metriken\n",
    "    for col in df.columns:\n",
    "        sem_type_row = df_sem_types[df_sem_types['Spalte'] == col].iloc[0] if col in df_sem_types['Spalte'].values else {'Semantischer Typ': str(df[col].dtype)}\n",
    "        \n",
    "        # Die Anzahl der duplizierten Eintr√§ge in der Spalte (df.shape[0] - df[col].nunique())\n",
    "        duplicate_count = len(df) - df[col].nunique()\n",
    "        \n",
    "        consolidated_data[col] = {\n",
    "            'Spalte': col,\n",
    "            'Semantischer Typ': sem_type_row['Semantischer Typ'],\n",
    "            'Urspr√ºnglicher Datentyp': str(df[col].dtype),\n",
    "            'Einzigartige Werte': df[col].nunique(),\n",
    "            'Duplizierte Werte (Anzahl)': duplicate_count, # Hinzugef√ºgt\n",
    "            'Fehlende Werte (Anzahl)': df[col].isnull().sum(),\n",
    "            'Fehlende Werte (%)': round(df[col].isnull().sum() / len(df) * 100, 2),\n",
    "            \n",
    "            # Statistische und ML-Metriken (Standardm√§√üig NaN)\n",
    "            'Min': np.nan, '25% (Q1)': np.nan, 'Median': np.nan, '75% (Q3)': np.nan, 'Max': np.nan,\n",
    "            'Skewness': np.nan,\n",
    "            'Ausrei√üer (IQR-Anzahl)': 0,\n",
    "        }\n",
    "    # 2. Numerische Metriken und ML-Analyse\n",
    "    numeric_relevant_types = {'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)', 'Integer', 'Boolean', 'Boolean(NaN)'}\n",
    "    \n",
    "    for _, row in df_sem_types.iterrows():\n",
    "        column = row['Spalte']\n",
    "        semantic_type = row['Semantischer Typ']\n",
    "        series = df[column]\n",
    "\n",
    "        # F√ºhre numerische Analysen nur f√ºr als numerisch erkannte Spalten durch\n",
    "        if semantic_type in numeric_relevant_types or pd.api.types.is_numeric_dtype(series):\n",
    "            try:\n",
    "                # Verwenden von to_numeric mit errors='coerce' ist sicherer f√ºr gemischte Typen\n",
    "                numeric_series = pd.to_numeric(series, errors='coerce').dropna()\n",
    "                \n",
    "                if not numeric_series.empty:\n",
    "                    q1, median, q3 = numeric_series.quantile([0.25, 0.5, 0.75])\n",
    "                    min_val = numeric_series.min()\n",
    "                    max_val = numeric_series.max()\n",
    "\n",
    "                    skewness = round(numeric_series.skew(), 2)\n",
    "                    \n",
    "                    Q1_o, Q3_o = numeric_series.quantile([0.25, 0.75])\n",
    "                    IQR = Q3_o - Q1_o\n",
    "                    lower_bound = Q1_o - 1.5 * IQR\n",
    "                    upper_bound = Q3_o + 1.5 * IQR\n",
    "                    outliers_count = ((numeric_series < lower_bound) | (numeric_series > upper_bound)).sum()\n",
    "                    \n",
    "                    consolidated_data[column].update({\n",
    "                        'Min': round(min_val, 2),\n",
    "                        '25% (Q1)': round(q1, 2),\n",
    "                        'Median': round(median, 2),\n",
    "                        '75% (Q3)': round(q3, 2),\n",
    "                        'Max': round(max_val, 2),\n",
    "                        'Skewness': skewness,\n",
    "                        'Ausrei√üer (IQR-Anzahl)': int(outliers_count)\n",
    "                    })\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # 3. KONSOLIDIERTE AUSGABE\n",
    "    \n",
    "    final_df = pd.DataFrame(list(consolidated_data.values()))\n",
    "    \n",
    "    # Spaltenreihenfolge f√ºr die Lesbarkeit anpassen (Duplizierte Werte hinzugef√ºgt)\n",
    "    column_order = [\n",
    "        'Spalte', 'Semantischer Typ', 'Urspr√ºnglicher Datentyp', 'Einzigartige Werte',\n",
    "        'Duplizierte Werte (Anzahl)', \n",
    "        'Fehlende Werte (Anzahl)', 'Fehlende Werte (%)', \n",
    "        'Min', '25% (Q1)', 'Median', '75% (Q3)', 'Max', 'Skewness',\n",
    "        'Ausrei√üer (IQR-Anzahl)',\n",
    "    ]\n",
    "    final_df = final_df[[col for col in column_order if col in final_df.columns]]\n",
    "    \n",
    "    # NaN-Werte und 0-Werte f√ºr Nicht-Numerische durch \"-\" ersetzen\n",
    "    for stat_col in ['Min', '25% (Q1)', 'Median', '75% (Q3)', 'Max', 'Skewness']:\n",
    "        final_df[stat_col] = final_df[stat_col].apply(lambda x: '-' if pd.isna(x) else x)\n",
    "        \n",
    "    final_df['Ausrei√üer (IQR-Anzahl)'] = final_df.apply(\n",
    "        lambda row: '-' if row['Semantischer Typ'] not in numeric_relevant_types and not pd.api.types.is_numeric_dtype(df[row['Spalte']]) else int(row['Ausrei√üer (IQR-Anzahl)']) if row['Ausrei√üer (IQR-Anzahl)'] is not None else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    # FINALE AUSGABE\n",
    "    print('*' * 10, 'KONSOLIDIERTE DATENQUALIT√ÑTS-ANALYSE', '*' * 10)\n",
    "    print(' '*120 + (('-->'+'  ')*3))\n",
    "    # HIER WIRD DIE GESAMTANZAHL DER DUPLIKATE IN ZEILEN ANGEZEIGT:\n",
    "    print(f\"Form (Zeilen, Spalten): {df.shape} | Duplikate (Zeilen): {df.duplicated().sum()}\")\n",
    "    print(\"-\" * 50)\n",
    "    # Ausgabe der finalen, konsolidierten Tabelle\n",
    "    print(final_df.to_string())\n",
    "    print(' '*120 + (('-->'+'  ')*3))\n",
    "    print('*' * 50)\n",
    "\n",
    "# HAUPTTEIL DES SKRIPTS\n",
    "# df_check = df_Cleaning wird zu Beginn des Musters gesetzt.\n",
    "if 'df_check' in locals() and isinstance(df_check, pd.DataFrame):\n",
    "    muster_df_consolidated_view(df_check)\n",
    "else:\n",
    "    print(\"Bitte laden Sie Ihren Datensatz in einen Pandas DataFrame!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëê Unique zur reinen Anzeige der Unique-Werte (Inhalt) pro Spalte\n",
    "\n",
    "df_check = df\n",
    "\n",
    "print(\"UNIVERSAL: Inhalt der einzigartigen Werte pro Spalte\")\n",
    "\n",
    "for col in df_check.columns:\n",
    "    series = df_check[col]\n",
    "    \n",
    "    if series.nunique(dropna=False) > 50:\n",
    "        print(f\"\\nSpalte '{col}' (Datentyp: {series.dtype}): **Unique-Anzeige √ºbersprungen (Kardinalit√§t > 50)**\")\n",
    "        continue\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Spalte: '{col}' (Datentyp: {series.dtype})\")\n",
    "    unique_list = series.unique()\n",
    "    \n",
    "    # Verwendung von to_string(), um das gesamte Array √ºbersichtlich auszugeben\n",
    "    print(pd.Series(unique_list).to_string(header=False, index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10699416",
   "metadata": {},
   "source": [
    "# Datenbereinigung und Vorbereitung \n",
    "- Die historischen Darlehensdaten m√ºssen ges√§ubert, fehlende Werte imputiert und Features (Merkmale) erstellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cleaning-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Datenbereinigung\n",
    "# ============================================================\n",
    "df_clear = df.copy()\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Spalten mit fehlenden Werten identifizieren\n",
    "cols_with_missing = df_clear.columns[df_clear.isnull().any()].tolist()\n",
    "\n",
    "# Durch die Spalten iterieren und fehlende Werte behandeln\n",
    "for col in cols_with_missing:\n",
    "    # Spalte hinzuf√ºgen, um anzuzeigen, ob der Wert urspr√ºnglich NaN war\n",
    "    df_clear[f'is_na_{col}'] = df_clear[col].isnull()\n",
    "    \n",
    "    # Fehlende Werte f√ºllen\n",
    "    if df_clear[col].dtype == 'object':\n",
    "        # Kategoriale Spalten mit dem Modus f√ºllen\n",
    "        mode_val = df_clear[col].mode()[0]\n",
    "        df_clear[col].fillna(mode_val, inplace=True)\n",
    "    else:\n",
    "        # Numerische Spalten mit dem Median f√ºllen (robuster gegen√ºber Ausrei√üern)\n",
    "        median_val = df_clear[col].median()\n",
    "        df_clear[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Duplikate entfernen\n",
    "df_clear.drop_duplicates(inplace=True)\n",
    "\n",
    "# Datentypen korrigieren\n",
    "df_clear['dependents'] = df_clear['dependents'].str.replace('3+', '3').astype(int)\n",
    "\n",
    "# Konvertiere 'gender', 'married', 'education', 'self_employed', 'property_area' in kategoriale Typen\n",
    "for col in ['gender', 'married', 'education', 'self_employed', 'property_area']:\n",
    "    df_clear[col] = df_clear[col].astype('category')\n",
    "    \n",
    "print('Datenbereinigung abgeschlossen.')\n",
    "print(f'Form des bereinigten DataFrames: {df_clear.shape}')\n",
    "print('Fehlende Werte nach der Bereinigung:')\n",
    "print(df_clear.isnull().sum())\n",
    "print('Info zum bereinigten DataFrame:')\n",
    "df_clear.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da072ef8",
   "metadata": {},
   "source": [
    "1. Datenbereinigung (df_clear)\n",
    "Das Ziel dieser Phase war es, einen sauberen und nutzbaren Datensatz f√ºr die weitere Analyse zu erstellen.\n",
    "\n",
    "Behandlung fehlender Werte:\n",
    "\n",
    "Fehlende Werte in numerischen Spalten (z.B. loan_amount) wurden durch den Median ersetzt. Der Median ist robuster gegen√ºber Ausrei√üern als der Mittelwert.\n",
    "Fehlende Werte in kategorialen Spalten (z.B. gender, married) wurden durch den Modus (den am h√§ufigsten vorkommenden Wert) ersetzt.\n",
    "F√ºr jede Spalte, in der Werte ersetzt wurden, wurde eine neue is_na_**-Spalte** (z.B. is_na_loan_amount) hinzugef√ºgt. Diese Boolean(NaN)-Spalte dient als \"Ged√§chtnis\" und erm√∂glicht es dem Modell, zu lernen, ob das urspr√ºngliche Fehlen eines Wertes eine eigene Bedeutung hat.\n",
    "Entfernung von Duplikaten: Doppelte Zeilen wurden aus dem Datensatz entfernt, um die Datenqualit√§t zu verbessern und Verzerrungen im Modell zu vermeiden.\n",
    "\n",
    "Korrektur von Datentypen: Die Spalte dependents wurde von einem Text- in einen numerischen Typ umgewandelt, um sie f√ºr mathematische Berechnungen und das Modelltraining nutzbar zu machen. Andere textbasierte Spalten mit einer begrenzten Anzahl von Werten (gender, married, etc.) wurden in den Datentyp category umgewandelt, um die Effizienz der Datenverarbeitung zu verbessern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f07c6",
   "metadata": {},
   "source": [
    "# Explorative Datenanalyse (EDA): \n",
    "- Verstehen, welche Merkmale die Ausfallwahrscheinlichkeit am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Explorative Datenanalyse\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Verteilung der numerischen Merkmale\n",
    "numerical_cols = df_clear.select_dtypes(include=np.number).columns.tolist()\n",
    "df_clear[numerical_cols].hist(bins=15, figsize=(15, 10), layout=(-1, 3))\n",
    "plt.suptitle('Verteilung der numerischen Merkmale')\n",
    "plt.show()\n",
    "\n",
    "# Verteilung der kategorialen Merkmale\n",
    "categorical_cols = df_clear.select_dtypes(include=['category']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.countplot(x=col, data=df_clear, hue='loan_status')\n",
    "    plt.title(f'Verteilung von {col} nach Loan Status')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Korrelationsmatrix\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(df_clear[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Korrelationsmatrix der numerischen Merkmale')\n",
    "plt.show()\n",
    "\n",
    "# Diesen Code in Zelle [6] hinzuf√ºgen, um den fehlenden Plot zu erzeugen\n",
    "plt.figure(figsize=(4, 2))\n",
    "sns.countplot(x='credit_history', data=df_clear, hue='loan_status')\n",
    "plt.title('Einfluss der Kredithistorie auf den Kreditstatus')\n",
    "plt.xlabel('Kredithistorie (0 = Nein, 1 = Ja)')\n",
    "plt.ylabel('Anzahl der Antr√§ge')\n",
    "plt.legend(title='Kreditstatus', labels=['Abgelehnt', 'Genehmigt'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72baaf8",
   "metadata": {},
   "source": [
    "2. Explorative Datenanalyse (EDA)\n",
    "In dieser Phase wurden Visualisierungen erstellt, um Muster und Zusammenh√§nge in den Daten zu erkennen.\n",
    "\n",
    "Verteilung der numerischen Merkmale (Histogramme):\n",
    "\n",
    "Bedeutung: Diese Diagramme zeigen die H√§ufigkeitsverteilung f√ºr jede numerische Spalte (z.B. applicant_income, loan_amount).\n",
    "Erkenntnisse: Man konnte sehen, dass Merkmale wie applicant_income und coapplicant_income rechtsschief sind, was bedeutet, dass die meisten Antragsteller ein geringeres Einkommen haben, w√§hrend einige wenige ein sehr hohes Einkommen aufweisen. Dies war ein wichtiger Hinweis f√ºr das Feature Engineering.\n",
    "Verteilung der kategorialen Merkmale nach loan_status (Balkendiagramme):\n",
    "\n",
    "Bedeutung: Diese Diagramme vergleichen die Verteilung von Kategorien (z.B. Education, Property_Area) f√ºr genehmigte (loan_status = 1) und abgelehnte (loan_status = 0) Kredite.\n",
    "Erkenntnisse: Hier konnten wir erste Hinweise auf wichtige Merkmale erkennen. Zum Beispiel war der Anteil genehmigter Kredite bei Antragstellern mit einer Kredithistorie (credit_history = 1.0) deutlich h√∂her als bei denen ohne.\n",
    "Korrelationsmatrix (Heatmap):\n",
    "\n",
    "Bedeutung: Diese Heatmap visualisiert die Korrelation (lineare Beziehung) zwischen den numerischen Merkmalen. Werte nahe +1 oder -1 deuten auf eine starke positive bzw. negative Beziehung hin.\n",
    "Erkenntnisse: Es wurde eine erwartete positive Korrelation zwischen loan_amount und applicant_income festgestellt. Wichtig war auch zu sehen, dass keine extrem hohen Korrelationen zwischen den unabh√§ngigen Merkmalen bestanden, was f√ºr viele Modelle vorteilhaft ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02ce7d0",
   "metadata": {},
   "source": [
    "# Feature Engineering: \n",
    "- Erstellen neuer, aussagekr√§ftiger Variablen (z.B. Schulden-zu-Einkommen-Verh√§ltnis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e9723",
   "metadata": {},
   "source": [
    "\n",
    "####Entfernen aller Spalten, die mit 'is_na_' beginnen\n",
    "is_na_cols = [col for col in df_customer_data.columns if col.startswith('is_na_')]\n",
    "cols_to_drop.extend(is_na_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† FEATURE ENGINEERING + ML-BEREINIGUNG\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 0Ô∏è‚É£ - Ausgangsdaten\n",
    "df_customer_data = df_clear.copy()\n",
    "\n",
    "# 1Ô∏è‚É£ - Feature Engineering\n",
    "df_customer_data['total_income'] = (\n",
    "    df_customer_data['applicant_income'] + df_customer_data['coapplicant_income']\n",
    ")\n",
    "\n",
    "# Vermeide Division durch Null bei total_income\n",
    "df_customer_data['loan_to_income_ratio'] = df_customer_data['loan_amount'] / np.where(\n",
    "    df_customer_data['total_income'] == 0, np.nan, df_customer_data['total_income']\n",
    ")\n",
    "\n",
    "df_customer_data['loan_term_to_income_ratio'] = df_customer_data['loan_amount_term'] / np.where(\n",
    "    df_customer_data['total_income'] == 0, np.nan, df_customer_data['total_income']\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ - One-Hot-Encoding (OHE)\n",
    "categorical_cols = df_customer_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "if 'loan_id' in categorical_cols:\n",
    "    categorical_cols.remove('loan_id')  # Identifikationsspalte nicht encodieren\n",
    "\n",
    "df_customer_data = pd.get_dummies(df_customer_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# 3Ô∏è‚É£ - Spaltenbereinigung\n",
    "cols_to_drop = []\n",
    "\n",
    "# Entfernen aller Spalten, die mit 'is_na_' beginnen\n",
    "is_na_cols = [col for col in df_customer_data.columns if col.startswith('is_na_')]\n",
    "cols_to_drop.extend(is_na_cols)\n",
    "\n",
    "# Entfernen der identifizierten Spalten\n",
    "df_customer_data = df_customer_data.drop(columns=list(set(cols_to_drop)), errors='ignore')\n",
    "\n",
    "\n",
    "# 4Ô∏è‚É£ - Numerische und bin√§re Spalten erkennen\n",
    "cols_to_scale = []\n",
    "binary_cols = []\n",
    "\n",
    "for col in df_customer_data.columns:\n",
    "    if col == 'loan_id':\n",
    "        continue  # ID-Spalte √ºberspringen\n",
    "    unique_vals = df_customer_data[col].dropna().unique()\n",
    "    if all(val in [0, 1] for val in unique_vals) and len(unique_vals) <= 2:\n",
    "        binary_cols.append(col)\n",
    "    else:\n",
    "        cols_to_scale.append(col)\n",
    "\n",
    "# 5Ô∏è‚É£ - Imputation (Median) der numerischen Spalten\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_customer_data[cols_to_scale] = imputer.fit_transform(df_customer_data[cols_to_scale])\n",
    "\n",
    "# 6Ô∏è‚É£ - Skalierung der numerischen Spalten\n",
    "scaler = StandardScaler()\n",
    "df_customer_data[cols_to_scale] = scaler.fit_transform(df_customer_data[cols_to_scale])\n",
    "\n",
    "# 7Ô∏è‚É£ - Abschluss\n",
    "print(\"‚úÖ Feature Engineering & ML-Bereinigung abgeschlossen.\")\n",
    "print(f\"‚û°Ô∏è Skalierte Features: {len(cols_to_scale)}\")\n",
    "print(f\"‚û°Ô∏è Bin√§re Features: {len(binary_cols)}\")\n",
    "print(f\"‚û°Ô∏è Gesamtspalten: {df_customer_data.shape[1]}\")\n",
    "print(f\"‚û°Ô∏è DataFrame-Form: {df_customer_data.shape}\")\n",
    "\n",
    "# Vorschau\n",
    "print(df_customer_data.head())\n",
    "\n",
    "# Das DataFrame df_customer_data ist nun ML-bereit (f√ºr Klassifikation, Regression, Clustering etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11016ebb",
   "metadata": {},
   "source": [
    "3. Feature Engineering (df_customer_data)\n",
    "Hier wurden neue, aussagekr√§ftigere Merkmale aus den vorhandenen Daten erstellt, um dem Modell zu helfen, bessere Vorhersagen zu treffen.\n",
    "\n",
    "total_income: Die Summe aus applicant_income und coapplicant_income. Dieses Merkmal ist oft aussagekr√§ftiger als die einzelnen Einkommen.\n",
    "loan_to_income_ratio: Das Verh√§ltnis von loan_amount zu total_income. Dies ist eine sehr wichtige Kennzahl, die das relative Risiko eines Kredits im Verh√§ltnis zum Einkommen des Antragstellers misst.\n",
    "One-Hot-Encoding: Die kategorialen Spalten wurden in ein numerisches Format umgewandelt, das von den Machine-Learning-Modellen verarbeitet werden kann. Dabei wird f√ºr jede Kategorie eine neue Spalte erstellt, die entweder 0 oder 1 enth√§lt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cc59c",
   "metadata": {},
   "source": [
    "# Modelltraining und -auswahl: \n",
    "- Training verschiedener ML-Modelle und Auswahl des besten Modells, das eine hohe Vorhersagekraft hat (z.B. √ºber Metriken wie AUC oder Gini-Koeffizient)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e7f533",
   "metadata": {},
   "source": [
    "### Das Modell (Maschinelles Lernen)\n",
    "Das Modell ‚Äì in den meisten F√§llen eine Klassifikationsaufgabe im Maschinellen Lernen (ML) ‚Äì lernt aus den historischen Daten, welche Merkmale am st√§rksten mit einem Ausfall korrelieren.\n",
    "\n",
    "Standard-ML-Verfahren: Traditionell wird oft die Logistische Regression verwendet (wegen ihrer einfachen Interpretierbarkeit und regulatorischen Akzeptanz). Heutzutage kommen aber auch komplexere Modelle wie Entscheidungsb√§ume, Random Forests oder Gradient Boosting Machines (GBM) zum Einsatz.\n",
    "\n",
    "Ergebnis: Das Modell liefert f√ºr jeden neuen Antragsteller einen Score oder eine Ausfallwahrscheinlichkeit (P \n",
    "Ausfall\n",
    "‚Äã\n",
    " ‚àà[0,1]). Je h√∂her der Wert, desto risikoreicher der Kredit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering-App mit Dropdown-Men√ºs und erweiterter Ellbogen-Einstellung\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from typing import Dict, Any, List, Union\n",
    "from scipy.stats import rankdata\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "from ipywidgets import VBox, Dropdown, Button, Output, Label, HTML, SelectMultiple, IntText \n",
    "\n",
    "# Die Warnungen aus der vorherigen Zelle werden hier ignoriert, da sie im bereinigten Code behoben sind\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# WICHTIGE DEFINITION: df_ML (der vom benutzer ausgewehlte Aktuelle Dataframe zur ML)\n",
    "df_ML = pd.DataFrame() \n",
    "\n",
    "class ClusteringApp:\n",
    "    def __init__(self, original_df: pd.DataFrame, comparison_column: Union[str, List[str]] = None, treatment_map: Dict[str, str] = None, \n",
    "                 elbow_style: str = 'bx-', n_init: Any = 'auto', cat_cleaning_map: Dict[str, str] = None):\n",
    "        self.original_df = original_df.copy() \n",
    "        self.df = original_df.copy() \n",
    "        if comparison_column is None:\n",
    "             self.comparison_col = []\n",
    "        elif isinstance(comparison_column, str):\n",
    "             self.comparison_col = [comparison_column]\n",
    "        else:\n",
    "             self.comparison_col = comparison_column \n",
    "             \n",
    "        self.treatment_map = treatment_map if treatment_map is not None else {}\n",
    "        self.cat_cleaning_map = cat_cleaning_map if cat_cleaning_map is not None else {} \n",
    "        self.elbow_style = elbow_style \n",
    "        self.n_init = n_init \n",
    "        self.X_scaled = None\n",
    "        self.results = {}\n",
    "        self.best_method = None\n",
    "        self.selected_method_from_viz = None\n",
    "        self.X_final_features = None \n",
    "        self.out = Output()   \n",
    "        self.max_k = 2\n",
    "\n",
    "    def _normalize_metric(self, scores: pd.Series, ascending: bool = True) -> pd.Series:\n",
    "        if scores.empty:\n",
    "            return pd.Series(0, index=scores.index)\n",
    "        ranks = rankdata(scores.fillna(scores.min() if ascending else scores.max()), method='dense')\n",
    "        min_rank = ranks.min()\n",
    "        max_rank = ranks.max()\n",
    "        if max_rank == min_rank:\n",
    "            return pd.Series(1.0, index=scores.index)\n",
    "        normalized_ranks = (ranks - min_rank) / (max_rank - min_rank)\n",
    "        if not ascending:\n",
    "            return 1 - normalized_ranks\n",
    "        else:\n",
    "            return pd.Series(normalized_ranks, index=scores.index)\n",
    "\n",
    "    # SCHRITT 1: DATENVORBEREITUNG \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Bereitet die Daten vor (NaN-Behandlung, Bin√§r-Umwandlung, One-Hot-Encoding, Skalierung) mit Robustheits-Sicherheitsnetz.\"\"\"\n",
    "        with self.out:\n",
    "            clear_output()\n",
    "            print(\"==================================================\")\n",
    "            print(\"\\u27A1 SCHRITT 1: DATENVORBEREITUNG\")\n",
    "            rows_initial = len(self.df)\n",
    "            cols_initial = len(self.df.columns)\n",
    "            print(f\"Start-Dimensionen (Gesampelt): {rows_initial} Zeilen, {cols_initial} Spalten.\")\n",
    "            \n",
    "            # Drop der Vergleichsspalten (jetzt Liste)\n",
    "            X = self.df.drop(columns=self.comparison_col, errors='ignore') \n",
    "            if self.comparison_col:\n",
    "                print(f\"\\u2139 Folgende Spalten werden f√ºr Clustering ignoriert: **{', '.join(self.comparison_col)}**.\")\n",
    "            \n",
    "            X_cleaned = X.copy()\n",
    "            num_cols = X_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            cat_cols = X_cleaned.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.difference(num_cols).tolist()\n",
    "            binary_cols = []\n",
    "            \n",
    "            # Bin√§re Konvertierung\n",
    "            print(\"\\n\\u27A1 Bin√§re Kategoriale Spalten umwandeln (z.B. Ja/Nein, M/W, Yes/No):\")\n",
    "            for col in cat_cols.copy():\n",
    "                unique_count = X_cleaned[col].nunique(dropna=True)\n",
    "                if unique_count == 2 or X_cleaned[col].dtype == 'bool':                \n",
    "                    if X_cleaned[col].dtype == 'object' or X_cleaned[col].dtype == 'category':\n",
    "                        unique_vals = [str(v).lower() for v in X_cleaned[col].dropna().unique()]\n",
    "                        \n",
    "                        if len(unique_vals) == 2:\n",
    "                            # F√ºgt 'yes'/'no' Erkennung hinzu\n",
    "                            if 'yes' in unique_vals or 'no' in unique_vals:\n",
    "                                val_1 = [v for v in unique_vals if v in ['yes', 'no']][0]\n",
    "                                val_0 = [v for v in unique_vals if v not in ['yes', 'no']][0] if len([v for v in unique_vals if v not in ['yes', 'no']]) > 0 else ([v for v in unique_vals if v != val_1][0] if len(unique_vals) == 2 else unique_vals[0])\n",
    "                                \n",
    "                                # Annahme: 'yes' oder 'wahr' soll 1 sein, alles andere 0\n",
    "                                if val_1 == 'yes':\n",
    "                                    mapping = {val_0: 0, val_1: 1}\n",
    "                                elif 'false' in unique_vals and 'true' in unique_vals:\n",
    "                                    mapping = {'false': 0, 'true': 1}\n",
    "                                else:\n",
    "                                    # Generische 0/1 Zuordnung\n",
    "                                    mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "                                    \n",
    "                                # Anwendung der Mapping\n",
    "                                X_cleaned[col] = X_cleaned[col].map(mapping).astype('Int64')\n",
    "                                print(f\"  - Spalte '{col}': Bin√§r zu **Numerisch (0/1)** konvertiert. (Muster: {mapping})\")\n",
    "\n",
    "                            else:\n",
    "                                # Generische 0/1 Zuordnung f√ºr zwei Werte\n",
    "                                mapping = {X_cleaned[col].dropna().unique()[0]: 0, X_cleaned[col].dropna().unique()[1]: 1}\n",
    "                                X_cleaned[col] = X_cleaned[col].map(mapping).astype('Int64')\n",
    "                                print(f\"  - Spalte '{col}': Bin√§r zu **Numerisch (0/1)** konvertiert.\")\n",
    "\n",
    "                        else:\n",
    "                            # Wenn es scheinbar bin√§r war, aber die Z√§hlung nach DropNA mehr als 2 Werte ergab (unwahrscheinlich)\n",
    "                            X_cleaned[col] = pd.factorize(X_cleaned[col])[0]\n",
    "                            print(f\"  - Spalte '{col}': Bin√§r zu **Faktorisierter Numerisch** konvertiert (mehr als 2 Werte).\")\n",
    "                    \n",
    "                    elif X_cleaned[col].dtype == 'bool':\n",
    "                         X_cleaned[col] = X_cleaned[col].astype(int)\n",
    "                         print(f\"  - Spalte '{col}': Bool zu **Int (0/1)** konvertiert.\")\n",
    "\n",
    "                    cat_cols.remove(col)\n",
    "                    num_cols.append(col)\n",
    "                    binary_cols.append(col)\n",
    "\n",
    "            print(f\"\\u2139 Verbleibende Kategoriale Spalten f√ºr One-Hot-Encoding: {len(cat_cols)}\")\n",
    "\n",
    "            # KATEGORIALE BEREINIGUNG ANWENDEN (TEMP_Clear_ML)\n",
    "            print(\"\\n\\u27A1 Anwendung der Kategorialen Bereinigungs-Muster:\")\n",
    "            for col, action in self.cat_cleaning_map.items():\n",
    "                if col not in X_cleaned.columns: continue \n",
    "\n",
    "                if action == 'TEMP_Clear_ML':\n",
    "                    if X_cleaned[col].dtype == 'object':\n",
    "                        X_cleaned[col] = X_cleaned[col].astype(str)\n",
    "                        X_cleaned[col] = X_cleaned[col].str.replace(' ', '_', regex=False)\n",
    "                        X_cleaned[col] = X_cleaned[col].replace('nan', 'MISSING')\n",
    "                        print(f\"  - Kategorisch '{col}': **TEMP_Clear_ML** (Spaces to _, 'nan' zu 'MISSING') angewendet.\")\n",
    "                elif action == 'none':\n",
    "                    pass\n",
    "            \n",
    "            # SPALTENSPEZIFISCHE BEHANDLUNG FEHLENDER WERTE \n",
    "            rows_to_drop = []\n",
    "            print(\"\\n\\u27A1 Behandlung von NaN-Werten (gem√§√ü Auswahl):\")         \n",
    "            for col, treatment in self.treatment_map.items():\n",
    "                if col not in X_cleaned.columns: continue\n",
    "                if X_cleaned[col].isnull().sum() == 0 and col not in binary_cols: continue   \n",
    "                if treatment == 'median' or treatment == 'mean':\n",
    "                    if col in num_cols:\n",
    "                        value = X_cleaned[col].median() if treatment == 'median' else X_cleaned[col].mean()\n",
    "                        X_cleaned[col] = X_cleaned[col].fillna(value)\n",
    "                        print(f\"  - Numerisch '{col}': NaNs mit **{treatment.upper()}** imputiert.\")\n",
    "                elif treatment == 'mode':\n",
    "                    if col in cat_cols or col in binary_cols: \n",
    "                        mode_val = X_cleaned[col].mode()\n",
    "                        if not mode_val.empty:\n",
    "                            X_cleaned[col] = X_cleaned[col].fillna(mode_val[0])\n",
    "                            print(f\"  - Kategorisch/Bin√§r '{col}': NaNs mit **MODUS ('{mode_val[0]}')** imputiert.\")\n",
    "                elif treatment == 'drop_row':\n",
    "                    rows_to_drop.extend(X_cleaned[X_cleaned[col].isnull()].index.tolist())\n",
    "                    print(f\"  - '{col}': Zeilen mit NaNs zur Entfernung markiert.\")\n",
    "            \n",
    "            rows_to_drop = list(set(rows_to_drop))\n",
    "            X_cleaned = X_cleaned.drop(rows_to_drop, errors='ignore')\n",
    "            # Stellt sicher, dass numerische Spalten wirklich numerisch sind\n",
    "            X_cleaned.loc[:, num_cols] = X_cleaned[num_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "            print(\"\\n\\u27A1 Feature Engineering (One-Hot Encoding):\")\n",
    "            cat_cols_final = X_cleaned.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "            X_final = pd.get_dummies(X_cleaned, columns=cat_cols_final, drop_first=True)\n",
    "            \n",
    "            X_final_features = X_final.select_dtypes(include=[np.number])\n",
    "            self.X_final_features = X_final_features\n",
    "            \n",
    "            # WICHTIG: self.df wird auf die gesampelten/bereinigten Zeilen reduziert (deren Index nun den Labels entspricht)\n",
    "            self.df = self.df.loc[X_final_features.index].copy()\n",
    "            \n",
    "            print(\"\\n\\u27A1 Skalierung (StandardScaler):\")\n",
    "            scaler = StandardScaler()\n",
    "            self.X_scaled = scaler.fit_transform(X_final_features)            \n",
    "            \n",
    "            if self.X_scaled.size == 0 or len(self.X_scaled) < 2:\n",
    "                print(\"‚ùå FEHLER: Nicht gen√ºgend Datenpunkte nach der Bereinigung. Clustering unm√∂glich.\")\n",
    "                return False\n",
    "            \n",
    "            rows_final = len(self.X_scaled)\n",
    "            print(f\"‚úÖ Daten fertig skaliert. Analysiert {rows_final} Zeilen und {self.X_scaled.shape[1]} Features.\")\n",
    "            print(f\"  ({rows_initial - rows_final} Zeilen wurden aufgrund von 'drop_row' oder NaNs entfernt).\")\n",
    "            return True\n",
    "\n",
    "    # SCHRITT 2: OPTIMALE K-BESTIMMUNG (Ellbogen & Silhouette) \n",
    "    def show_elbow(self):\n",
    "        \"\"\"Generiert den Ellbogen-Plot und den Silhouette-Score-Plot zur Bestimmung von k.\"\"\"\n",
    "        with self.out:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(\"\\u27A1 SCHRITT 2: OPTIMALE K-BESTIMMUNG (Ellbogen & Silhouette)\")            \n",
    "            distortions = []\n",
    "            silhouette_scores = []\n",
    "            self.max_k = min(11, len(self.X_scaled)) \n",
    "            K = range(1, self.max_k)            \n",
    "            \n",
    "            if len(K) < 2: \n",
    "                print(\"‚ùå Nicht genug Datenpunkte f√ºr die k-Methoden (ben√∂tigt mindestens 2).\")\n",
    "                return False\n",
    "            \n",
    "            K_for_sil = range(2, self.max_k)\n",
    "            if not K_for_sil:\n",
    "                print(\"‚ùå Nicht genug Datenpunkte f√ºr die k-Methoden (ben√∂tigt k >= 2).\")\n",
    "                return False\n",
    "\n",
    "            print(f\"\\u2139 Berechne Metriken f√ºr k von 1 bis {self.max_k-1} (n_init={self.n_init}).....\")\n",
    "            \n",
    "            for k in K:\n",
    "                try:\n",
    "                    n_init_val = self.n_init if self.n_init == 'auto' else int(self.n_init)\n",
    "                    km = KMeans(n_clusters=k, random_state=42, n_init=n_init_val, max_iter=300)\n",
    "                    km.fit(self.X_scaled)\n",
    "                    distortions.append(km.inertia_)\n",
    "                    \n",
    "                    if k > 1 and len(set(km.labels_)) > 1:\n",
    "                        score = silhouette_score(self.X_scaled, km.labels_)\n",
    "                        silhouette_scores.append(score)\n",
    "                    else:\n",
    "                        silhouette_scores.append(None)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è KMeans Fehler bei k={k}: {e}\")\n",
    "                    distortions.append(None)\n",
    "                    silhouette_scores.append(None)                    \n",
    "            \n",
    "            K_valid_elbow = [k for k, d in zip(K, distortions) if d is not None]\n",
    "            distortions_valid = [d for d in distortions if d is not None]            \n",
    "            \n",
    "            K_valid_sil = [k for k, d in zip(K[1:], silhouette_scores[1:]) if d is not None]\n",
    "            sil_valid = [d for d in silhouette_scores[1:] if d is not None]\n",
    "\n",
    "            if not K_valid_elbow or not K_valid_sil:\n",
    "                print(\"‚ùå Keine g√ºltigen Metrik-Werte berechnet.\")\n",
    "                return False\n",
    "\n",
    "            plt.figure(figsize=(6, 2)) # Adjusted figure size for better display\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(K_valid_elbow, distortions_valid, self.elbow_style) \n",
    "            plt.xlabel(\"Anzahl der Muster (k)\")\n",
    "            plt.ylabel(\"Distortion (Inertia)\")\n",
    "            plt.title(\"Ellbogen-Methode\")\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(K_valid_sil, sil_valid, \"ro-\")\n",
    "            plt.xlabel(\"Anzahl der Muster (k)\")\n",
    "            plt.ylabel(\"Silhouette Score\")\n",
    "            plt.title(\"Silhouette-Analyse\")\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"‚úÖ Plots generiert. Bitte w√§hlen Sie nun k (Schritt 5) von 2 bis {self.max_k-1}.\")\n",
    "            return True\n",
    "\n",
    "\n",
    "    # SCHRITT 3: METHODENVERGLEICH (Mit Total Score) \n",
    "    def compare_methods(self, k: int):\n",
    "        \"\"\"F√ºhrt verschiedene Clustering-Methoden aus und vergleicht sie mittels Total Score.\"\"\"\n",
    "        with self.out:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(f\"\\u27A1 SCHRITT 3: METHODENVERGLEICH & TOTAL SCORE (k = {k})\")\n",
    "            self.results = {}\n",
    "            n_samples = self.X_scaled.shape[0]\n",
    "            \n",
    "            # W√§hlt Affinity basierend auf der Stichprobengr√∂√üe f√ºr SpectralClustering\n",
    "            spectral_affinity = 'nearest_neighbors' if n_samples > 1000 else 'rbf'\n",
    "            if spectral_affinity == 'nearest_neighbors':\n",
    "                 print(f\"  \\u26A0 SpectralClustering: {n_samples} Zeilen, verwende 'nearest_neighbors' f√ºr Performance.\")\n",
    "                 \n",
    "            n_init_val = self.n_init if self.n_init == 'auto' else int(self.n_init)\n",
    "\n",
    "            methods_to_run = {\n",
    "                \"KMeans\": KMeans(n_clusters=k, random_state=42, n_init=n_init_val, max_iter=500), \n",
    "                \"GMM (Gaussian Mixture)\": GaussianMixture(n_components=k, random_state=42, n_init=10, max_iter=500),\n",
    "                \"Agglomerative\": AgglomerativeClustering(n_clusters=k, linkage='ward'),\n",
    "                \"Spectral\": SpectralClustering(n_clusters=k, random_state=42, n_init=10, affinity=spectral_affinity),\n",
    "                \"Birch\": Birch(n_clusters=k)\n",
    "            }\n",
    "\n",
    "            for name, model in methods_to_run.items():\n",
    "                try:\n",
    "                    if name == \"GMM (Gaussian Mixture)\":\n",
    "                        model.fit(self.X_scaled)\n",
    "                        labels = model.predict(self.X_scaled)\n",
    "                    else:\n",
    "                        labels = model.fit_predict(self.X_scaled)\n",
    "                        \n",
    "                    num_clusters = len(set(labels))\n",
    "                    \n",
    "                    if num_clusters > 1 and num_clusters == k: # Ensure we have k clusters and not fewer\n",
    "                        \n",
    "                        # Labels neu zuordnen nach N (Gr√∂√üter wird 1)\n",
    "                        df_temp = pd.DataFrame({'Labels': labels})\n",
    "                        sizes = df_temp['Labels'].value_counts().sort_values(ascending=False)\n",
    "                        \n",
    "                        # Nur Cluster > -1 (kein Rauschen) neu zuordnen\n",
    "                        non_noise_sizes = sizes[sizes.index != -1]\n",
    "                        # Startet die Nummerierung bei 1 (Gr√∂√üter Cluster = 1)\n",
    "                        new_mapping = {old_label: new_label for new_label, old_label in enumerate(non_noise_sizes.index, start=1)}\n",
    "                        \n",
    "                        # Rauschen (-1) bleibt -1\n",
    "                        if -1 in sizes.index:\n",
    "                            new_mapping[-1] = -1 \n",
    "\n",
    "                        sorted_labels = df_temp['Labels'].map(new_mapping).fillna(-1).astype(int).values\n",
    "                        \n",
    "                        # FINALE Speicherung der sortierten Labels und Metriken\n",
    "                        result_dict = {\n",
    "                            \"Labels\": sorted_labels, # <--- HIER WERDEN DIE SORTIERTEN LABELS GESPEICHERT (Gr√∂√üter = 1)\n",
    "                            \"Original_Labels\": labels, # <- ORIGINAL Labels (0, 1, 2...) f√ºr Kontingenztabelle in Schritt 6\n",
    "                            \"Silhouette\": silhouette_score(self.X_scaled, labels), \n",
    "                            \"CH\": calinski_harabasz_score(self.X_scaled, labels),\n",
    "                            \"DB\": davies_bouldin_score(self.X_scaled, labels)\n",
    "                        }\n",
    "                        \n",
    "                        # Speichere das Modell selbst, falls es Cluster-Zentren hat (f√ºr Feature Importance)\n",
    "                        if name in [\"KMeans\", \"GMM (Gaussian Mixture)\"]:\n",
    "                            result_dict[\"Model\"] = model\n",
    "                        \n",
    "                        self.results[name] = result_dict\n",
    "                        print(f\"  - **{name}** erfolgreich berechnet. Muster: {num_clusters} (Sortiert 1, 2, ...)\")\n",
    "                    else:\n",
    "                        print(f\"  - **{name}** √ºbersprungen (Musterzahl {num_clusters} entspricht nicht k={k} oder ist trivial).\")\n",
    "                        \n",
    "                except MemoryError:\n",
    "                    print(f\"  ‚ùå **{name}** Fehler: Nicht genug Speicher. √úberspringen.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå **{name}** Fehler: Unerwarteter Fehler: {type(e).__name__}. √úberspringen.\")\n",
    "                    \n",
    "            # DBSCAN-Spezialfall\n",
    "            print(\"\\n\\u27A1 Spezialfall: DBSCAN\")\n",
    "            try:\n",
    "                neigh = NearestNeighbors(n_neighbors=5).fit(self.X_scaled)\n",
    "                distances, _ = neigh.kneighbors(self.X_scaled)\n",
    "                eps = np.percentile(distances[:, -1], 90) \n",
    "                \n",
    "                db = DBSCAN(eps=eps, min_samples=5).fit(self.X_scaled)\n",
    "                labels = db.labels_\n",
    "                \n",
    "                labels_clean = labels[labels != -1]\n",
    "                X_scaled_clean = self.X_scaled[labels != -1]\n",
    "                \n",
    "                num_valid_clusters = len(set(labels_clean))\n",
    "                \n",
    "                if num_valid_clusters > 1 and len(labels_clean) >= 2: \n",
    "                    # Labels neu zuordnen nach N (Gr√∂√üter wird 1)\n",
    "                    df_temp = pd.DataFrame({'Labels': labels})\n",
    "                    sizes = df_temp['Labels'].value_counts().sort_values(ascending=False)\n",
    "                    \n",
    "                    non_noise_sizes = sizes[sizes.index != -1]\n",
    "                    new_mapping = {old_label: new_label for new_label, old_label in enumerate(non_noise_sizes.index, start=1)}\n",
    "                    \n",
    "                    if -1 in sizes.index:\n",
    "                        new_mapping[-1] = -1 \n",
    "\n",
    "                    sorted_labels = df_temp['Labels'].map(new_mapping).fillna(-1).astype(int).values\n",
    "                    \n",
    "                    self.results[\"DBSCAN\"] = {\n",
    "                        \"Labels\": sorted_labels,\n",
    "                        \"Original_Labels\": labels,\n",
    "                        \"Silhouette\": silhouette_score(X_scaled_clean, labels_clean),\n",
    "                        \"CH\": calinski_harabasz_score(X_scaled_clean, labels_clean),\n",
    "                        \"DB\": davies_bouldin_score(X_scaled_clean, labels_clean)\n",
    "                    }\n",
    "                    print(f\"  - **DBSCAN** erfolgreich berechnet. Muster: {num_valid_clusters} (+ Rauschen). (Sortiert 1, 2, ...)\")\n",
    "                else:\n",
    "                    print(f\"  - **DBSCAN** √ºbersprungen (Zu wenig g√ºltige Muster nach Rauschen-Entfernung oder zu wenig Punkte).\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                 print(f\"  ‚ùå **DBSCAN** Fehler: Unerwarteter Fehler: {type(e).__name__}. √úberspringen.\")\n",
    "\n",
    "            if not self.results:\n",
    "                print(\"‚ùå Keine g√ºltigen Clustering-Ergebnisse zum Vergleichen.\")\n",
    "                return\n",
    "\n",
    "            res_df = pd.DataFrame([\n",
    "                {\"Methode\": k, **{k:v for k,v in v.items() if k not in ['Labels', 'Original_Labels', 'Model']}} for k, v in self.results.items()\n",
    "            ]).drop(columns=[]).set_index(\"Methode\")\n",
    "            \n",
    "            res_df['Norm_Silhouette'] = self._normalize_metric(res_df['Silhouette'], ascending=True)\n",
    "            res_df['Norm_CH'] = self._normalize_metric(res_df['CH'], ascending=True)\n",
    "            res_df['Norm_DB'] = self._normalize_metric(res_df['DB'], ascending=False)\n",
    "            \n",
    "            res_df['Total Score'] = res_df[['Norm_Silhouette', 'Norm_CH', 'Norm_DB']].mean(axis=1).round(3)\n",
    "            \n",
    "            self.best_method = res_df['Total Score'].idxmax()\n",
    "            self.selected_method_from_viz = self.best_method\n",
    "\n",
    "            comparison_cols = ['Silhouette', 'CH', 'DB', 'Total Score']\n",
    "            res_df = res_df[comparison_cols]\n",
    "            res_df = res_df.sort_values(by=\"Total Score\", ascending=False)\n",
    "            \n",
    "            res_df['Silhouette'] = res_df['Silhouette'].round(3)\n",
    "            res_df['DB'] = res_df['DB'].round(3)\n",
    "            res_df['CH'] = res_df['CH'].round(0).astype(int)\n",
    "            \n",
    "            print(\"\\nüèÜ Metriken-Vergleich (Normalisiert + Total Score):\")\n",
    "            display(res_df)\n",
    "            \n",
    "            print(f\"\\n**Ergebnis:** Die beste Methode (nach Total Score) ist **{self.best_method}**.\")\n",
    "            self.results[self.best_method]['Total Score'] = res_df.loc[self.best_method]['Total Score']\n",
    "\n",
    "\n",
    "    # SCHRITT 4: VISUALISIERUNGS-STEUERUNG \n",
    "    def show_visualization_controls(self):\n",
    "        \"\"\"Zeigt die Steuerelemente f√ºr Visualisierung und Analyse an.\"\"\"\n",
    "        if not self.results:\n",
    "            return\n",
    "            \n",
    "        method_dd = Dropdown(options=list(self.results.keys()),\n",
    "                             value=self.best_method, description=\"Methode:\")\n",
    "        show_btn = Button(description=\"Plot & Analyse anzeigen\")\n",
    "        viz_out = Output()\n",
    "        \n",
    "        with self.out:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(\"\\u27A1 SCHRITT 4: VISUALISIERUNG & INTERPRETATION\")\n",
    "            print(\"W√§hlen Sie eine Methode, um die Muster in 2D (PCA-reduziert) zu visualisieren und die deskriptive Analyse zu sehen.\")\n",
    "            display(VBox([method_dd, show_btn, viz_out]))\n",
    "        \n",
    "        def viz(b):\n",
    "            method = method_dd.value\n",
    "            self.selected_method_from_viz = method\n",
    "            with viz_out:\n",
    "                clear_output()\n",
    "                self.plot_clusters(method)\n",
    "                self.analyze_clusters(method)\n",
    "                \n",
    "        show_btn.on_click(viz)\n",
    "\n",
    "        return self.results[self.best_method][\"Labels\"] if self.best_method else None\n",
    "\n",
    "    # SCHRITT 5: DESKRIPTIVE MUSTER-ANALYSE (Interpretation) - MIT SORTIERUNG NACH N\n",
    "    def analyze_clusters(self, method: str):\n",
    "        \"\"\"Zeigt deskriptive Statistiken der Cluster-Zentren (Interpretation).\"\"\"\n",
    "        if method not in self.results:\n",
    "            return\n",
    "\n",
    "        labels = self.results[method][\"Labels\"]\n",
    "        \n",
    "        df_labeled = self.df.copy() \n",
    "        df_labeled['Cluster'] = labels \n",
    "        \n",
    "        cols_to_analyze = df_labeled.columns.drop(['Cluster'] + self.comparison_col, errors='ignore')\n",
    "\n",
    "        analysis_data = []\n",
    "        sorted_clusters = sorted(df_labeled['Cluster'].unique(), key=lambda x: (x == -1, x)) \n",
    "        \n",
    "        for cluster_id in sorted_clusters:\n",
    "            cluster_data = {'Cluster': str(cluster_id)}\n",
    "            subset = df_labeled[df_labeled['Cluster'] == cluster_id]\n",
    "            N = len(subset)\n",
    "            cluster_data['N (Anzahl)'] = N\n",
    "\n",
    "            for col in cols_to_analyze:\n",
    "                dtype = subset[col].dtype\n",
    "                \n",
    "                original_col_name = col\n",
    "                if original_col_name in self.original_df.columns:\n",
    "                    original_subset = self.original_df.loc[subset.index, original_col_name]\n",
    "                    dtype_orig = original_subset.dtype\n",
    "                    \n",
    "                    # ------------------------------------------------\n",
    "                    # Verwende is_numeric_dtype aus pandas.api.types\n",
    "                    # ------------------------------------------------\n",
    "                    if is_numeric_dtype(dtype_orig): \n",
    "                        cluster_data[f'{original_col_name} (Mean)'] = original_subset.mean().round(2)\n",
    "                    elif dtype_orig == 'object' or dtype_orig == 'category' or dtype_orig == 'bool':\n",
    "                        mode_val = original_subset.mode()\n",
    "                        mode_str = str(mode_val[0]) if not mode_val.empty else 'N/A'\n",
    "                        if mode_str.lower() in ['yes', 'no']: # Handle common boolean string representations\n",
    "                             mode_str = mode_str.lower().replace('yes', 'ja').replace('no', 'nein') \n",
    "                        cluster_data[f'{original_col_name} (Mode)'] = mode_str\n",
    "                else:\n",
    "                    if np.issubdtype(dtype, np.number):\n",
    "                         cluster_data[f'{col} (Mean)'] = subset[col].mean().round(2)\n",
    "                    elif dtype == 'object' or dtype == 'category' or dtype == 'bool':\n",
    "                        mode_val = subset[col].mode()\n",
    "                        mode_str = str(mode_val[0]) if not mode_val.empty else 'N/A'\n",
    "                        if mode_str.lower() in ['yes', 'no']: # Handle common boolean string representations\n",
    "                             mode_str = mode_str.lower().replace('yes', 'ja').replace('no', 'nein') \n",
    "                        cluster_data[f'{col} (Mode)'] = mode_str\n",
    "                        \n",
    "            analysis_data.append(cluster_data)\n",
    "\n",
    "        df_analysis = pd.DataFrame(analysis_data).set_index('Cluster')\n",
    "        \n",
    "        non_noise_clusters = df_analysis[df_analysis.index != '-1']\n",
    "        noise_cluster = df_analysis[df_analysis.index == '-1'] if '-1' in df_analysis.index else pd.DataFrame()\n",
    "        \n",
    "        df_analysis = pd.concat([non_noise_clusters, noise_cluster])\n",
    "\n",
    "\n",
    "        with self.out:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(f\"üìä DESKRIPTIVE ANALYSE ({method}) - SORTIERT NACH GR√ñSSE (N)\")\n",
    "            print(\"==================================================\")\n",
    "            print(f\"Statistiken der Muster-Zentren f√ºr **{method}** basierend auf Original-Features:\")\n",
    "            display(df_analysis)\n",
    "            print(\"\\u2139 Die Interpretation dieser Werte (z.B. hohe Einkommen, niedriges Alter) definiert die Segmente.\")\n",
    "            return df_analysis  \n",
    "\n",
    "    # Plot-Logik (KORRIGIERT: explained_ratio_ zu explained_variance_ratio_)\n",
    "    def plot_clusters(self, method: str):\n",
    "        \"\"\"Reduziert auf 2D mittels PCA und plottet die Muster.\"\"\"\n",
    "        if method not in self.results:\n",
    "            print(f\"‚ùå Methode '{method}' nicht gefunden.\")\n",
    "            return\n",
    "\n",
    "        with self.out:\n",
    "            print(f\"\\n\\u2139 Generiere 2D-Plot f√ºr **{method}** (PCA-Reduktion)...\")\n",
    "\n",
    "        labels = self.results[method][\"Labels\"]\n",
    "        \n",
    "        pca = PCA(n_components=2)\n",
    "        pca_result = pca.fit_transform(self.X_scaled)\n",
    "        \n",
    "        hue_labels = [str(l) for l in labels]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        if method == \"DBSCAN\" and -1 in labels:\n",
    "            noise_indices = (labels == -1)\n",
    "            core_indices = (labels != -1)\n",
    "            \n",
    "            sns.scatterplot(x=pca_result[core_indices, 0], y=pca_result[core_indices, 1], \n",
    "                            hue=[str(l) for l in labels[core_indices]], \n",
    "                            palette=\"viridis\", legend=\"full\", s=70)\n",
    "            \n",
    "            plt.scatter(pca_result[noise_indices, 0], pca_result[noise_indices, 1], \n",
    "                        c='gray', marker='x', s=50, label=\"Rauschen (-1)\")\n",
    "            \n",
    "            handles, labels_plt = plt.gca().get_legend_handles_labels()\n",
    "            plt.legend(handles, labels_plt, title=\"Muster\")\n",
    "        else:\n",
    "            sns.scatterplot(x=pca_result[:, 0], y=pca_result[:, 1], hue=hue_labels, palette=\"viridis\", legend=\"full\")\n",
    "            \n",
    "        plt.title(f\"Muster: {method} (PCA-reduziert)\")\n",
    "        plt.xlabel(f\"PCA Komponente 1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "        # FEHLER BEHOBEN: explained_ratio_ durch explained_variance_ratio_ ersetzt\n",
    "        plt.ylabel(f\"PCA Komponente 2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "        plt.show()\n",
    "        \n",
    "        with self.out:\n",
    "             print(\"‚úÖ Visualisierung abgeschlossen.\")\n",
    "    \n",
    "    \n",
    "    # NEU: Universelle Methode zur Berechnung und Visualisierung der Merkmalswichtigkeit\n",
    "    # PASST DIE AUSWERTUNG F√úR ALLE CLUSTERING-METHODEN AN\n",
    "    def _calculate_and_plot_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Berechnet die Wichtigkeit der Features basierend auf der Standardabweichung \n",
    "        der Cluster-Mittelwerte in den zugewiesenen Daten. Dies ist universell f√ºr \n",
    "        ALLE Methoden mit Cluster-Zuweisungen.\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.out:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(\"üí° MERKMALSWICHTIGKEIT (Universeller Vergleich)\")\n",
    "            print(\"==================================================\")\n",
    "            \n",
    "        found_valid_models = False\n",
    "        \n",
    "        for method in self.results.keys():\n",
    "            \n",
    "            # 1. Pr√ºfe, ob Zuweisungen existieren\n",
    "            if \"Labels\" not in self.results[method] or self.results[method][\"Labels\"] is None:\n",
    "                continue\n",
    "                \n",
    "            # Erstelle die Cluster-Zuweisung als Series mit korrektem Index\n",
    "            cluster_assignments = pd.Series(self.results[method][\"Labels\"], index=self.df.index)\n",
    "            \n",
    "            # DBSCAN Rauschen (-1) herausfiltern, da es kein Cluster ist\n",
    "            valid_indices = cluster_assignments != -1\n",
    "            \n",
    "            # Nur relevante Daten f√ºr die Berechnung verwenden (muss auf dem skalierten X_final_features basieren)\n",
    "            X_data = self.X_final_features.loc[valid_indices]\n",
    "            assignments = cluster_assignments.loc[valid_indices]\n",
    "\n",
    "            # Pr√ºfe, ob nach Filterung noch Daten vorhanden sind\n",
    "            if X_data.empty or len(assignments.unique()) < 2:\n",
    "                with self.out:\n",
    "                    print(f\"\\n--- ERGEBNIS: {method} ---\")\n",
    "                    print(\"‚ö†Ô∏è Nicht gen√ºgend g√ºltige Cluster-Daten vorhanden (z.B. nur Rauschen oder nur ein Cluster nach Filterung).\")\n",
    "                continue\n",
    "    \n",
    "            found_valid_models = True\n",
    "            \n",
    "            try:\n",
    "                # 2. Berechnung der 'Synthetischen Cluster-Zentren' (Mittelwerte der Cluster)\n",
    "                # Grouping des skalierten Feature-DataFrames nach der Zuweisung und Berechnung des Mittelwerts.\n",
    "                cluster_means = X_data.groupby(assignments).mean()\n",
    "                \n",
    "                # 3. Berechnung der Wichtigkeit\n",
    "                # Die Wichtigkeit ist die Standardabweichung der Mittelwerte √ºber die Cluster hinweg.\n",
    "                feature_std = cluster_means.std(axis=0)\n",
    "                \n",
    "                # Feature-Namen\n",
    "                feature_names = self.X_final_features.columns.tolist()\n",
    "                \n",
    "                # Erstelle DataFrame f√ºr die Visualisierung\n",
    "                df_importance = pd.DataFrame({\n",
    "                    'Merkmal': feature_names,\n",
    "                    'Wichtigkeit (Std. der Cluster-Mittelwerte)': feature_std.values\n",
    "                }).sort_values(by='Wichtigkeit (Std. der Cluster-Mittelwerte)', ascending=False).head(15)\n",
    "    \n",
    "                # 4. Ausgabe und Plot\n",
    "                with self.out:\n",
    "                    print(f\"\\n--- ERGEBNIS: {method} ---\")\n",
    "                    print(\"Die Wichtigkeit basiert auf der Standardabweichung der Cluster-Mittelwerte in den zugewiesenen Daten (universelle Methode).\")\n",
    "                    \n",
    "                    plt.figure(figsize=(4, 3))\n",
    "                    sns.barplot(x='Wichtigkeit (Std. der Cluster-Mittelwerte)', y='Merkmal', data=df_importance, palette=\"plasma\")\n",
    "                    plt.title(f\"Einflussreichste Merkmale f√ºr die Muster-Bildung ({method})\")\n",
    "                    plt.xlabel(\"Wichtigkeit (Standardabweichung der Cluster-Mittelwerte)\")\n",
    "                    plt.ylabel(\"Merkmal\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                with self.out:\n",
    "                    print(f\"\\n‚ùå FEHLER bei der Berechnung/Visualisierung der Merkmalswichtigkeit f√ºr {method}: {type(e).__name__}.\")\n",
    "    \n",
    "        if not found_valid_models:\n",
    "             with self.out:\n",
    "                 print(\"‚ö†Ô∏è Es wurden keine Methoden mit g√ºltigen Cluster-Zuweisungen gefunden, um die universelle Merkmalswichtigkeit zu berechnen.\")\n",
    "\n",
    "    # NEU: Methode zur Cluster-Zuordnung (Cluster_ML) und Vergleich (TEMP_Clear_ML) - KORRIGIERT\n",
    "    def execute_assignment_and_comparison(self, solution_col_name: str, df_ML_ref: pd.DataFrame, selected_method: str = None):\n",
    "        \"\"\"F√ºhrt Cluster-Zuweisung (Cluster_ML) und optional den Vergleich (TEMP_Clear_ML) durch.\"\"\"\n",
    "        \n",
    "        global df_ML\n",
    "        df_ML = df_ML_ref\n",
    "        \n",
    "        # KORREKTUR: Verwende die manuell gew√§hlte Methode, ansonsten die beste Methode\n",
    "        assignment_method = selected_method if selected_method in self.results else self.best_method\n",
    "\n",
    "        if assignment_method is None:\n",
    "            print(\"‚ùå FEHLER: Keine Methode zur Zuordnung gefunden. Zuordnung nicht m√∂glich.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n==================================================\")\n",
    "        print(f\"üìä SCHRITT 7: MUSTER-ZUORDNUNG & VERGLEICH (Methode: {assignment_method})\")\n",
    "        print(\"==================================================\")\n",
    "        \n",
    "        # 1. Cluster_ML Zuweisung (Labels 1, 2, 3...)\n",
    "        sorted_labels = self.results[assignment_method][\"Labels\"]\n",
    "        cluster_series_sorted = pd.Series(sorted_labels, index=self.df.index) # Use self.df.index for alignment\n",
    "        \n",
    "        df_ML['Cluster_ML'] = pd.Series(pd.NA, index=df_ML.index, dtype='Int64') \n",
    "        valid_indices = df_ML.index.intersection(cluster_series_sorted.index)\n",
    "        df_ML.loc[valid_indices, 'Cluster_ML'] = cluster_series_sorted.loc[valid_indices].values \n",
    "        \n",
    "        print(f\"‚úÖ Spalte **'Cluster_ML'** (Sortierte IDs: 1, 2, 3...) in **df_ML** eingebunden.\")\n",
    "\n",
    "        # 2. Deskriptive Profilanalyse (Jetzt als Teil der Ausf√ºhrung)\n",
    "        self.perform_descriptive_analysis(df_ML)\n",
    "\n",
    "        # 3. TEMP_Clear_ML Zuweisung (falls Vergleich gew√ºnscht)\n",
    "        if solution_col_name and solution_col_name != \"Kein Vergleich\":\n",
    "            \n",
    "            if solution_col_name not in df_ML.columns:\n",
    "                 print(f\"‚ùå FEHLER: Die gew√§hlte L√∂sungsspalte **'{solution_col_name}'** existiert nicht in df_ML.\")\n",
    "                 return\n",
    "\n",
    "            df_compare = df_ML.loc[self.df.index].copy() # Use self.df.index for alignment\n",
    "            \n",
    "            df_compare['Cluster_ML_Sample'] = df_compare['Cluster_ML'] \n",
    "\n",
    "            df_compare.dropna(subset=['Cluster_ML_Sample', solution_col_name], inplace=True)\n",
    "            \n",
    "            if df_compare.empty:\n",
    "                print(\"‚ùå WARNUNG: Keine √ºbereinstimmenden Zeilen f√ºr den Vergleich nach NaN-Drop im Sample.\")\n",
    "                return\n",
    "\n",
    "            # Kontingenztabelle und Zuordnung\n",
    "            contingency = pd.crosstab(df_compare[solution_col_name], df_compare['Cluster_ML_Sample'])\n",
    "            cost_matrix = -contingency.values\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            cluster_mapping = {contingency.columns[c]: contingency.index[r] for r, c in zip(row_ind, col_ind)}\n",
    "            \n",
    "            # NEUE KORREKTUR: Richtige Zuordnung vom Cluster zur Predominant Category\n",
    "            df_compare['Predominant_Category'] = df_compare['Cluster_ML_Sample'].map(cluster_mapping)\n",
    "            df_compare['TEMP_Clear_ML_Match'] = df_compare[solution_col_name] == df_compare['Predominant_Category']\n",
    "            \n",
    "            n_abw = (~df_compare['TEMP_Clear_ML_Match']).sum()\n",
    "            print(f\"\\n‚ùå Abweichungen vom dominanten Cluster ('{solution_col_name}'): **{n_abw}**\")\n",
    "\n",
    "            # Erstellen und Zuweisen der TEMP_Clear_ML-Spalte\n",
    "            temp_clear_ml_series = pd.Series(pd.NA, index=df_ML.index, dtype=object)\n",
    "            \n",
    "            temp_clear_ml_series.loc[df_compare.index] = df_compare.apply(\n",
    "                # Cluster_ML_Sample (Sortierte IDs) f√ºr die Textausgabe verwenden\n",
    "                lambda row: f\"{df_ML.loc[row.name, solution_col_name]}+Cluster_{int(row['Cluster_ML_Sample'])}\" if row['TEMP_Clear_ML_Match'] else f\"Cluster_{int(row['Cluster_ML_Sample'])}\",\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            # Rauschen (-1) (sortierte ID) auff√ºllen, wo noch keine Zuweisung erfolgte (Original ID -1)\n",
    "            noise_indices = df_ML.index.intersection(cluster_series_sorted[cluster_series_sorted == -1].index)\n",
    "            temp_clear_ml_series.loc[noise_indices] = temp_clear_ml_series.loc[noise_indices].fillna(\"Cluster_-1\")\n",
    "\n",
    "            df_ML['TEMP_Clear_ML'] = temp_clear_ml_series.loc[df_ML.index]\n",
    "            \n",
    "            print(f\"‚úÖ Spalte **'TEMP_Clear_ML'** in **df_ML** gespeichert (Vergleich mit '{solution_col_name}').\")\n",
    "            \n",
    "            # Ausgabe der Kontingenztabelle und Heatmap\n",
    "            print(\"\\n--- Kontingenztabelle f√ºr Vergleich ---\")\n",
    "            display(contingency)\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.heatmap(contingency, annot=True, fmt=\"d\", cmap=\"viridis\", linewidths=.5, linecolor='black')\n",
    "            plt.title(f\"Muster-Verteilung f√ºr '{solution_col_name}'\")\n",
    "            plt.ylabel(solution_col_name)\n",
    "            # NEU: Beschriftung angepasst\n",
    "            plt.xlabel(\"Cluster (Cluster_ML IDs)\")\n",
    "            plt.show()\n",
    "\n",
    "            # NEUE ANPASSUNG: Merkmalswichtigkeit f√ºr alle geeigneten Methoden vergleichen\n",
    "            self._calculate_and_plot_feature_importance()\n",
    "\n",
    "\n",
    "        # ============================================================== \n",
    "        # üîç NEU: KORRELATION DER FEATURES MIT DER LERNSPALTE\n",
    "        # ==============================================================\n",
    "        if solution_col_name and solution_col_name != \"Kein Vergleich\" and solution_col_name in df_ML.columns:\n",
    "            print(\"\\n==================================================\")\n",
    "            print(\"üìà KORRELATION DER FEATURES MIT DER LERNSPALTE\")\n",
    "            print(\"==================================================\")\n",
    "            try:\n",
    "                # Sicherstellen, dass Lernspalte numerisch ist (bei Kategorien wird factorize verwendet)\n",
    "                target_series_with_index = df_ML.loc[self.df.index, solution_col_name].copy()\n",
    "                if not np.issubdtype(target_series_with_index.dtype, np.number):\n",
    "                    # Korrektur: Index nach factorize wiederherstellen\n",
    "                    codes, _ = pd.factorize(target_series_with_index)\n",
    "                    target_series = pd.Series(codes, index=target_series_with_index.index)\n",
    "                else:\n",
    "                    target_series = target_series_with_index\n",
    "\n",
    "                # Numerische Featurematrix aus den verwendeten Merkmalen\n",
    "                X_features = pd.DataFrame(self.X_scaled, index=self.df.index, columns=self.X_final_features.columns)\n",
    "\n",
    "                # Berechne die Korrelation jeder Spalte mit der Lernspalte\n",
    "                corr_values = {}\n",
    "                for col in X_features.columns:\n",
    "                    try:\n",
    "                        # Korrektur: Direkt auf den Series mit korrektem Index arbeiten\n",
    "                        corr = X_features[col].corr(target_series)\n",
    "                        corr_values[col] = abs(corr) if pd.notna(corr) else 0.0\n",
    "                    except Exception:\n",
    "                        corr_values[col] = 0.0\n",
    "\n",
    "                corr_df = pd.DataFrame(list(corr_values.items()), columns=[\"Merkmal\", \"Korrelation\"])\n",
    "                corr_df = corr_df.sort_values(by=\"Korrelation\", ascending=False)\n",
    "\n",
    "                # Plot\n",
    "                plt.figure(figsize=(4, 3))\n",
    "                sns.barplot(x=\"Korrelation\", y=\"Merkmal\", data=corr_df.head(20), palette=\"coolwarm\")\n",
    "                plt.title(f\"Top-Korrelationen der Features zur Lernspalte '{solution_col_name}'\")\n",
    "                plt.xlabel(\"Korrelationsst√§rke (|r|)\")\n",
    "                plt.ylabel(\"Feature\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                print(\"‚úÖ Korrelationsanalyse abgeschlossen. Die Grafik zeigt die 20 wichtigsten Merkmale.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Fehler bei der Korrelationsanalyse: {type(e).__name__}: {e}\")\n",
    "        # ==============================================================\n",
    "\n",
    "        print(\"\\n‚úÖ Schritt 7 abgeschlossen. df_ML enth√§lt nun die Zuordnung(en).\")\n",
    "        \n",
    "    # NEU: Methode zur Durchf√ºhrung der deskriptiven Analyse in df_ML (Universell)\n",
    "    def perform_descriptive_analysis(self, df_ML_ref: pd.DataFrame):\n",
    "        \"\"\"Generiert die universelle, deskriptive Cluster-Analyse aus df_ML.\"\"\"\n",
    "        \n",
    "        df_analyzed = df_ML_ref[df_ML_ref['Cluster_ML'].notna()].copy()\n",
    "\n",
    "        if 'Cluster_ML' in df_analyzed.columns:\n",
    "            \n",
    "            cols_to_profile = df_analyzed.columns.drop(['Cluster_ML', 'TEMP_Clear_ML'] + self.comparison_col, errors='ignore').tolist()\n",
    "            \n",
    "            agg_operations = {'N (Anzahl)': ('Cluster_ML', 'size')}\n",
    "            \n",
    "            for col in cols_to_profile:\n",
    "                if col in self.original_df.columns: # Check if the column exists in the original df\n",
    "                    # Direct aggregation on original columns in df_ML, if available\n",
    "                    col_data = df_analyzed[col]\n",
    "\n",
    "                    if np.issubdtype(col_data.dtype, np.number):\n",
    "                        agg_operations[f'{col} (Mean)'] = (col, 'mean')\n",
    "                    elif col_data.dtype == 'object' or col_data.dtype == 'category' or col_data.dtype == 'bool':\n",
    "                        agg_operations[f'{col} (Mode)'] = (col, lambda x: x.mode()[0] if not x.mode().empty else 'N/A')\n",
    "                        \n",
    "            df_cluster_profile = df_analyzed.groupby('Cluster_ML').agg(**agg_operations)\n",
    "            \n",
    "            # Sortierung: Sortiert 1, 2, 3... vor dem Rauschen (-1)\n",
    "            cluster_order = sorted(df_cluster_profile.index.tolist(), key=lambda x: (x == -1, x))\n",
    "            df_cluster_profile = df_cluster_profile.reindex(cluster_order)\n",
    "            \n",
    "            # Sortiere Spalten f√ºr bessere Lesbarkeit\n",
    "            cols = ['N (Anzahl)'] + [col for col in df_cluster_profile.columns if col != 'N (Anzahl)']\n",
    "            df_cluster_profile = df_cluster_profile[cols]\n",
    "            \n",
    "            # Runde numerische Spalten\n",
    "            for col in df_cluster_profile.columns:\n",
    "                if ' (Mean)' in col:\n",
    "                     df_cluster_profile[col] = df_cluster_profile[col].round(2)\n",
    "            \n",
    "            # Ausgabe der Cluster-Tabelle\n",
    "            print(\"\\n--------------------------------------------------\")\n",
    "            print(\"üìä DESKRIPTIVE CLUSTER-PROFILE (Universelle Auswertung)\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "            print(\"Basis: Analysierter Dataframe **df_ML**. Cluster sind nach Gr√∂√üe sortiert (**Cluster 1 = Gr√∂√üte**).\")\n",
    "            display(df_cluster_profile)\n",
    "            \n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è WARNUNG: Keine Cluster-Spalte ('Cluster_ML') in df_ML gefunden. Profilierung √ºbersprungen.\")\n",
    "#--- \n",
    "# Clustering-Tool: Startet den interaktiven Prozess\n",
    "class ClusteringTool:\n",
    "    def __init__(self):\n",
    "        global df_ML\n",
    "        if 'df_ML' not in globals():\n",
    "            df_ML = pd.DataFrame()\n",
    "        self.available_dfs = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame) and name != 'df_ML'}\n",
    "        if 'dataframe' in globals() and 'dataframe' not in self.available_dfs:\n",
    "             self.available_dfs['dataframe'] = globals()['dataframe']        \n",
    "        self.selected_df_name = None \n",
    "        self.selected_df = None\n",
    "        self.sampled_df = None\n",
    "        self.comparison_col = [] \n",
    "        self.solution_col_name = None\n",
    "        self.treatment_map = {}\n",
    "        self.cat_cleaning_map = {}\n",
    "        self.elbow_style = 'bx-'\n",
    "        self.n_init = 'auto'\n",
    "        self.app = None \n",
    "        self.best_labels = None         \n",
    "        self.comparison_result_df = None\n",
    "\n",
    "        self.cleaning_options = {\n",
    "            \"Vorschl√§ge zur Vorverarbeitung f√ºr ML (TEMP_Clear_ML)\": \"TEMP_Clear_ML\", \n",
    "            \"Keine (Ausschlie√üen von der Verarbeitung)\": \"none\"\n",
    "        }\n",
    "        \n",
    "        self.all_steps_container = VBox([])         \n",
    "        display(self.all_steps_container)\n",
    "        self.setup_df_selection()\n",
    "\n",
    "    def _add_step_widgets(self, title: str, widgets_list: List[widgets.Widget], handler: callable, btn_label: str, btn_style: str = ''):\n",
    "        \"\"\"Erstellt einen robusten Step Container und f√ºgt ihn zum Haupt-Container hinzu.\"\"\"\n",
    "        all_widgets = [HTML(f\"<h3>{title}</h3>\")]\n",
    "        all_widgets.extend(widgets_list)\n",
    "        step_container = VBox(all_widgets) \n",
    "        btn = Button(description=btn_label, button_style=btn_style)\n",
    "        output_for_step = Output() \n",
    "        btn.on_click(lambda b: handler(b, output_for_step))\n",
    "        self.all_steps_container.children += (step_container, btn, output_for_step)\n",
    "        return btn, output_for_step\n",
    "    \n",
    "    def setup_df_selection(self):\n",
    "        if not self.available_dfs:\n",
    "            self.all_steps_container.children = (HTML(\"‚ùå **Keine DataFrames gefunden.** Bitte laden Sie einen DataFrame in die Umgebung.\"),)\n",
    "            return\n",
    "        df_options = list(self.available_dfs.keys())\n",
    "        df_dd = Dropdown(options=df_options, description=\"DataFrame:\")        \n",
    "        \n",
    "        def on_ok_click(b, output_for_step):\n",
    "            with output_for_step:\n",
    "                clear_output()\n",
    "                self.selected_df_name = df_dd.value \n",
    "                self.selected_df = self.available_dfs[self.selected_df_name].copy() \n",
    "                \n",
    "                global df_ML\n",
    "                df_ML = self.selected_df.copy() \n",
    "                \n",
    "                print(f\"‚úÖ DataFrame **'{self.selected_df_name}'** ausgew√§hlt und als Arbeits-Dataframe **df_ML** kopiert.\")\n",
    "            df_dd.disabled = True\n",
    "            b.disabled = True            \n",
    "            self.setup_column_selection()\n",
    "            \n",
    "        self._add_step_widgets(\"1Ô∏è‚É£ DataFrame ausw√§hlen (wird zu df_ML kopiert):\",[df_dd], on_ok_click,\"OK (DataFrame w√§hlen)\")\n",
    "\n",
    "    def setup_column_selection(self):\n",
    "        column_options = self.selected_df.columns.tolist()\n",
    "        col_dd = widgets.SelectMultiple(options=column_options, description=\"Auszuschlie√üende Spalten:\", rows=8, style={'description_width': 'initial'})\n",
    "        \n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.comparison_col = list(col_dd.value) \n",
    "            with output_for_step:\n",
    "                clear_output()\n",
    "                print(f\"‚úÖ Auszuschlie√üende Spalten (IDs, etc.): **{', '.join(self.comparison_col) if self.comparison_col else 'Keine'}**\")            \n",
    "            col_dd.disabled = True\n",
    "            b.disabled = True            \n",
    "            self.setup_sampling_selection() \n",
    "        \n",
    "        self._add_step_widgets(\"2Ô∏è‚É£ Spalten (Ausschluss vom Clustering) ausw√§hlen:\",\n",
    "            [Label(\"W√§hlen Sie eine oder mehrere Spalten (z.B. IDs, reine Textspalten), die **nicht** in das Clustering einbezogen werden sollen.\"), col_dd],\n",
    "            on_next_click,\n",
    "            \"Weiter zu Sampling-Auswahl\") \n",
    "        \n",
    "    # SCHRITT 3: DATENREDUZIERUNG (Input-Feld)\n",
    "    def setup_sampling_selection(self):\n",
    "        n_rows = len(self.selected_df)\n",
    "        row_input = IntText(\n",
    "            value=n_rows,\n",
    "            min=1,\n",
    "            max=n_rows,\n",
    "            step=1,\n",
    "            description='Max. Zeilen:',\n",
    "            disabled=False,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        def on_next_click(b, output_for_step):\n",
    "            target_rows = row_input.value\n",
    "            \n",
    "            if target_rows < 1 or target_rows > n_rows:\n",
    "                with output_for_step:\n",
    "                    clear_output()\n",
    "                    print(f\"‚ùå FEHLER: Ung√ºltige Zeilenzahl. W√§hlen Sie zwischen 1 und {n_rows}.\")\n",
    "                return\n",
    "            if target_rows < n_rows:\n",
    "                rate = target_rows / n_rows\n",
    "                self.sampled_df = self.selected_df.sample(n=target_rows, random_state=42)\n",
    "                with output_for_step:\n",
    "                    clear_output()\n",
    "                    print(f\"‚úÖ Sampling: **{target_rows} Zeilen** ausgew√§hlt ({rate*100:.2f}%).\")\n",
    "            else:\n",
    "                self.sampled_df = self.selected_df.copy()\n",
    "                with output_for_step:\n",
    "                    clear_output()\n",
    "                    print(\"‚úÖ Sampling: **Voller Datensatz** ausgew√§hlt.\")\n",
    "            row_input.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_nan_treatment_selection()  \n",
    "            \n",
    "        self._add_step_widgets(\n",
    "            \"3Ô∏è‚É£ Datenreduzierung (Sampling) ausw√§hlen:\",\n",
    "            [Label(f\"Geben Sie die maximale Anzahl der zu analysierenden Zeilen ein (Aktuelle Gr√∂√üe: {n_rows}).\"), row_input],\n",
    "            on_next_click,\n",
    "            \"Weiter zu Fehlerbehandlung\"\n",
    "        )\n",
    "    def setup_nan_treatment_selection(self):\n",
    "        df_check = self.sampled_df.drop(columns=self.comparison_col, errors='ignore')\n",
    "        nan_cols = df_check.columns[df_check.isnull().any()].tolist()\n",
    "        self.treatment_widgets = {}\n",
    "        widget_list = []\n",
    "        num_options = {\"Median (Empfohlen)\": 'median', \"Mittelwert (Mean)\": 'mean', \"Zeilen entfernen\": 'drop_row'}\n",
    "        cat_options = {\"Modus (Mode/H√§ufigster Wert)\": 'mode', \"Zeilen entfernen\": 'drop_row'}\n",
    "        if nan_cols:\n",
    "            for col in nan_cols:\n",
    "                col_dtype = df_check[col].dtype                \n",
    "                if np.issubdtype(col_dtype, np.number):\n",
    "                    options = num_options\n",
    "                    label = f\"üî¢ **{col}** (Numerisch, {df_check[col].isnull().sum()} NaNs):\"\n",
    "                    default_value = 'median'\n",
    "                elif col_dtype == bool or col_dtype == 'object' or col_dtype == 'category':\n",
    "                    options = cat_options\n",
    "                    label = f\"üî† **{col}** (Kategorisch/Bool, {df_check[col].isnull().sum()} NaNs):\"\n",
    "                    default_value = 'mode'\n",
    "                else:\n",
    "                    continue\n",
    "                dropdown = Dropdown(options=options, value=default_value, description=\"Methode:\")\n",
    "                self.treatment_widgets[col] = dropdown\n",
    "                widget_list.append(VBox([Label(label), dropdown]))\n",
    "        else:\n",
    "             widget_list.append(HTML(\"‚úÖ **Keine fehlenden Werte** in den Features gefunden.\"))\n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.treatment_map = {col: dd.value for col, dd in self.treatment_widgets.items()}\n",
    "            with output_for_step:\n",
    "                clear_output() \n",
    "                print(f\"‚úÖ NaN-Behandlungen gespeichert.\")\n",
    "            for dd in self.treatment_widgets.values():\n",
    "                 dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_categorical_cleaning()\n",
    "        self._add_step_widgets(\"4Ô∏è‚É£ Behandlung fehlender Werte pro Spalte ausw√§hlen:\",widget_list,on_next_click,\"Weiter zu Kategorialer Bereinigung\")\n",
    "\n",
    "    def setup_categorical_cleaning(self):\n",
    "        df_check = self.sampled_df.drop(columns=self.comparison_col, errors='ignore')\n",
    "        cat_cols = df_check.select_dtypes(include=[\"object\", \"category\"]).columns.tolist() \n",
    "        self.cleaning_widgets = {}\n",
    "        widget_list = []\n",
    "        \n",
    "        if cat_cols:\n",
    "            for col in cat_cols:\n",
    "                if df_check[col].nunique(dropna=True) <= 2: continue \n",
    "                label = f\"üî† **{col}** (Kategorisch/String):\"\n",
    "                dropdown = Dropdown(options=self.cleaning_options, \n",
    "                                    value=\"TEMP_Clear_ML\", \n",
    "                                    description=\"Bereinigungs-Art:\", style={'description_width': 'initial'})\n",
    "                self.cleaning_widgets[col] = dropdown\n",
    "                widget_list.append(VBox([Label(label), dropdown]))\n",
    "        else:\n",
    "            widget_list.append(HTML(\"‚úÖ **Keine nicht-bin√§ren kategorialen Spalten** f√ºr zus√§tzliche Bereinigung gefunden.\"))\n",
    "            \n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.cat_cleaning_map = {col: dd.value for col, dd in self.cleaning_widgets.items()}\n",
    "            with output_for_step:\n",
    "                clear_output() \n",
    "                print(f\"‚úÖ Kategoriale Bereinigungen gespeichert.\")\n",
    "                for col, action in self.cat_cleaning_map.items():\n",
    "                    if action == \"TEMP_Clear_ML\":\n",
    "                         print(f\"\\n‚úÖ **{col}** - ML-Bereinigung (**TEMP_Clear_ML**) wird **automatisch** in Schritt 1 angewendet.\")\n",
    "                    elif action == \"none\":\n",
    "                         print(f\"\\n‚úÖ **{col}** - **Ausschluss** von weiterer kategorialer Bereinigung (d.h. sie wird unbereinigt als One-Hot-Encoding verwendet).\")\n",
    "            \n",
    "            for dd in self.cleaning_widgets.values():\n",
    "                 dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_elbow_style_selection()\n",
    "            \n",
    "        self._add_step_widgets(\"4c: Kategoriale Bereinigung ausw√§hlen:\",\n",
    "            widget_list, on_next_click, \"Weiter zu Ellbogen-Stil-Einstellung\"\n",
    "        )\n",
    "    def setup_elbow_style_selection(self):\n",
    "        style_options = {\"Linie mit blauen 'x' Markern ('bx-')\": 'bx-', \"Nur blaue 'x' Marker (keine Linie) ('bx')\": 'bx',\"Dicke blaue Linie (kein Marker) ('b-')\": 'b-',\"Linie mit roten Punkten ('ro-')\": 'ro-',\"Nur rote Punkte (keine Linie) ('ro')\": 'ro'}\n",
    "        style_dd = Dropdown(options=style_options, value=self.elbow_style, description=\"Darstellungs-Stil:\", style={'description_width': 'initial'})\n",
    "        def run_n_init_selection(b, output_for_step):\n",
    "            self.elbow_style = style_dd.value\n",
    "            with output_for_step:\n",
    "                clear_output() \n",
    "                print(f\"‚úÖ Darstellungs-Stil: **'{self.elbow_style}'** ausgew√§hlt.\")\n",
    "            style_dd.disabled = True\n",
    "            b.disabled = True            \n",
    "            self.setup_n_init_selection()\n",
    "        self._add_step_widgets(\n",
    "            \"4a: Ellbogen-Plot Darstellungs-Stil einstellen (Visuelle Einstellung):\",\n",
    "            [Label(\"W√§hlen Sie den Matplotlib-Darstellungs-Muster f√ºr den Ellbogen-Plot.\"), style_dd],\n",
    "            run_n_init_selection, \"Weiter zu Ellbogen-Stabilit√§t (n_init)\" \n",
    "        )\n",
    "    def setup_n_init_selection(self):\n",
    "        n_init_options = { \"Auto (Standard, schnell)\": 'auto', \"10 (Stabil, glatterer Plot)\": 10, \"5 (Schnellerer Test)\": 5, \"20 (Sehr stabil, langsam)\": 20 }\n",
    "        default_val = self.n_init if self.n_init in n_init_options.values() else 'auto'\n",
    "        n_init_dd = Dropdown(options={k: v for k, v in n_init_options.items()}, value=default_val, description=\"n_init (KMeans-Initialisierungen):\", style={'description_width': 'initial'})\n",
    "        def run_plots(b, output_for_step):\n",
    "            self.n_init = n_init_dd.value\n",
    "            with output_for_step:\n",
    "                clear_output() \n",
    "                print(f\"‚úÖ n_init f√ºr KMeans: **'{self.n_init}'** ausgew√§hlt.\")\n",
    "            self._execute_preparation_and_plots()\n",
    "        self._add_step_widgets(\"4b: Ellbogen-Stabilit√§t (n_init) einstellen:\",[Label(\"Steuert die Anzahl der KMeans-L√§ufe.\"), n_init_dd], run_plots,\"OK (Daten vorbereiten & Ellbogen-Plots generieren)\" )\n",
    "\n",
    "    def _execute_preparation_and_plots(self):\n",
    "        self.app = ClusteringApp(\n",
    "            original_df=self.sampled_df, \n",
    "            comparison_column=self.comparison_col, \n",
    "            treatment_map=self.treatment_map, \n",
    "            elbow_style=self.elbow_style, \n",
    "            n_init=self.n_init,\n",
    "            cat_cleaning_map=self.cat_cleaning_map\n",
    "        )\n",
    "        if self.app.out not in self.all_steps_container.children:\n",
    "            self.all_steps_container.children += (self.app.out,)\n",
    "        if self.app.prepare_data():\n",
    "            if self.app.show_elbow():\n",
    "                self.setup_k_selection()\n",
    "    def setup_k_selection(self):\n",
    "        if self.app.X_scaled is None or len(self.app.X_scaled) < 2: return\n",
    "        k_options = list(range(2, self.app.max_k)) \n",
    "        if not k_options: return\n",
    "        default_k = min(4, k_options[-1]) \n",
    "        k_dd = Dropdown(options=k_options, value=default_k, description=\"Musterzahl k:\")\n",
    "        def run(b, output_for_step):\n",
    "            k = k_dd.value\n",
    "            with output_for_step:\n",
    "                clear_output() \n",
    "                print(f\"‚úÖ Auswahl: k={k}. Starte Methodenvergleich...\")\n",
    "            k_dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.app.compare_methods(k)\n",
    "            self.best_labels = self.app.show_visualization_controls()  \n",
    "            self.setup_comparison_selection()\n",
    "        self._add_step_widgets(\"5Ô∏è‚É£ Musterzahl (k) ausw√§hlen:\",[Label(\"Basierend auf den Plots (Schritt 2), w√§hlen Sie die gew√ºnschte Musterzahl 'k' ‡™∏‡™Ç‡™™‡™∞‡´ç‡™ï\"), k_dd], run, \"Clustering Methoden vergleichen & Visualisierung starten\")\n",
    "    \n",
    "    # SCHRITT 6: L√ñSUNGSSPALTE W√ÑHLEN - KORRIGIERT\n",
    "    def setup_comparison_selection(self):\n",
    "        all_cols = self.selected_df.columns.tolist()\n",
    "        comparison_options = [\"Kein Vergleich\"] + all_cols\n",
    "        solution_dd = Dropdown(options=comparison_options, \n",
    "                               value=\"Kein Vergleich\", \n",
    "                               description=\"L√∂sungsspalte (Validation Column):\", \n",
    "                               style={'description_width': 'initial'})\n",
    "        def run_comparison(b, output_for_step):\n",
    "            self.solution_col_name = solution_dd.value\n",
    "            with output_for_step:\n",
    "                clear_output()\n",
    "                if self.app.selected_method_from_viz is None:\n",
    "                    print(\"‚ùå FEHLER: Es wurde keine Methode zur Zuordnung gefunden. F√ºhren Sie **Schritt 5** erneut aus.\")\n",
    "                    return\n",
    "                print(f\"‚úÖ L√∂sungsspalte gew√§hlt: **'{self.solution_col_name}'**.\")\n",
    "                \n",
    "                solution_dd.disabled = True\n",
    "                b.disabled = True\n",
    "                \n",
    "                # KORREKTUR: Wenn \"Kein Vergleich\" gew√§hlt ist, √ºberspringe den Vergleichsschritt (implizit in Schritt 7)\n",
    "                if self.solution_col_name == \"Kein Vergleich\":\n",
    "                    print(\"‚û°Ô∏è **Kein Vergleich** ausgew√§hlt (Szenario A). Fahren Sie mit **Schritt 7** fort, um nur die Muster zuzuordnen.\")\n",
    "                else:\n",
    "                    print(\"‚û°Ô∏è Vergleichsspalte ausgew√§hlt (Szenario B). Fahren Sie mit **Schritt 7** fort, um die Muster zuzuordnen, den Vergleich und die **Merkmalswichtigkeit** durchzuf√ºhren.\")\n",
    "                    \n",
    "                self.setup_final_assignment()\n",
    "                \n",
    "        self._add_step_widgets(\"6Ô∏è‚É£ L√∂sungsspalte (Validation Column) w√§hlen:\",[Label(\"Diese Spalte wird f√ºr den Muster-Vergleich und die Generierung der Spalte **'TEMP_Clear_ML'** in df_ML verwendet.\"), solution_dd], run_comparison,\"OK (Vergleichs-Spalte speichern)\",btn_style='info')\n",
    "\n",
    "    # SCHRITT 7: MUSTER-ZUORDNUNG (Cluster_ML) UND VERGLEICH (TEMP_Clear_ML) DURCHF√úHREN\n",
    "    def setup_final_assignment(self):\n",
    "        def assign_musters_and_compare(b, output_for_step):\n",
    "            with output_for_step:\n",
    "                clear_output()\n",
    "                global df_ML\n",
    "                if 'df_ML' not in globals() or globals()['df_ML'].empty:\n",
    "                     df_ML = self.sampled_df.copy()\n",
    "                     globals()['df_ML'] = df_ML\n",
    "                     print(\"‚ÑπÔ∏è df_ML wurde aus dem Sample-DataFrame re-initialisiert.\")\n",
    "                if self.app is None or self.app.selected_method_from_viz is None:\n",
    "                    print(\"‚ùå FEHLER: ClusteringApp wurde nicht richtig initialisiert oder Schritt 5 wurde nicht ausgef√ºhrt.\")\n",
    "                    return\n",
    "                \n",
    "                self.app.execute_assignment_and_comparison(self.solution_col_name, df_ML, self.app.selected_method_from_viz)\n",
    "                \n",
    "                print(\"\\n‚úÖ Schritt 7 abgeschlossen. df_ML enth√§lt nun die Zuordnung(en).\")\n",
    "                print(\"‚û°Ô∏è Fahren Sie mit **Schritt 8** fort, um die finalen Spalten zuzuordnen und df_ML zu l√∂schen.\")\n",
    "            b.disabled = True\n",
    "            self.setup_final_cleanup()\n",
    "        \n",
    "        # Logik, die den Titel f√ºr den Button anpasst\n",
    "        if self.solution_col_name and self.solution_col_name != \"Kein Vergleich\":\n",
    "            label_text = \"Dieser Schritt f√ºgt **Cluster_ML** und **TEMP_Clear_ML** (Vergleich) in den Arbeits-DataFrame **df_ML** ein und generiert die **Merkmalswichtigkeit**.\"\n",
    "        else:\n",
    "            label_text = \"Dieser Schritt f√ºgt **Cluster_ML** in den Arbeits-DataFrame **df_ML** ein und generiert die finale deskriptive Cluster-Analyse (Kein Vergleich und keine Merkmalswichtigkeit).\"\n",
    "\n",
    "        self._add_step_widgets(\n",
    "            \"7Ô∏è‚É£ Cluster-Zuordnung (Cluster_ML) & Vergleich (TEMP_Clear_ML) durchf√ºhren:\",\n",
    "            [Label(f\"Cluster-Zuordnung erfolgt mit der in **Schritt 4** gew√§hlten Methode: **{self.app.selected_method_from_viz if self.app and self.app.selected_method_from_viz else 'Nicht gew√§hlt/Beste'}**.\"), \n",
    "             Label(label_text), \n",
    "             Label(\"Er generiert auch die finale deskriptive Cluster-Analyse.\")],\n",
    "            assign_musters_and_compare,\n",
    "            \"OK (Muster zuordnen & Analyse generieren)\",\n",
    "            btn_style='warning'\n",
    "        )\n",
    "    # SCHRITT 8: FINALE SPEICHERUNG UND BEREINIGUNG\n",
    "    def setup_final_cleanup(self):\n",
    "        assignment_options = { \" Muster in DataFrame einbinden (Spalte: 'Cluster_ML')\": \"assign\", \"Keine (Analyse abgeschlossen)\": \"none\"}\n",
    "        assign_dd = Dropdown(options=assignment_options, value=\"assign\", description=\"Zuordnung:\", style={'description_width': 'initial'})\n",
    "        def final_cleanup(b, output_for_step):\n",
    "            with output_for_step:\n",
    "                clear_output()\n",
    "                global df_ML \n",
    "                target_df_name = self.selected_df_name\n",
    "                \n",
    "                if assign_dd.value == \"assign\":\n",
    "                    if 'df_ML' not in globals() or globals()['df_ML'].empty or 'Cluster_ML' not in globals()['df_ML'].columns:\n",
    "                        print(\"‚ùå FEHLER: df_ML enth√§lt keine 'Cluster_ML' Spalte. F√ºhren Sie **Schritt 7** aus.\")\n",
    "                        return\n",
    "                    # 1. Zuweisung an das globale, vom Benutzer gew√§hlte DataFrame (Weiterarbeiten)\n",
    "                    if target_df_name in globals() and isinstance(globals()[target_df_name], pd.DataFrame):\n",
    "                        df_target = globals()[target_df_name]\n",
    "                        # Sicherstellen, dass die Spalten erstellt werden (falls sie im Original-DF fehlen)\n",
    "                        df_target['Cluster_ML'] = pd.Series(pd.NA, index=df_target.index, dtype='Int64') \n",
    "                        if 'TEMP_Clear_ML' not in df_target.columns:\n",
    "                            df_target['TEMP_Clear_ML'] = pd.Series(pd.NA, index=df_target.index, dtype=object)\n",
    "                        # Werte aus df_ML √ºbertragen\n",
    "                        df_target['Cluster_ML'].update(df_ML['Cluster_ML'])\n",
    "                        if 'TEMP_Clear_ML' in df_ML.columns:\n",
    "                            df_target['TEMP_Clear_ML'].update(df_ML['TEMP_Clear_ML'])\n",
    "                        globals()[target_df_name] = df_target\n",
    "                        target_name_display = f\"**{target_df_name}**\"\n",
    "                        print(f\"‚úÖ Finale Spalten **'Cluster_ML'** und optional **'TEMP_Clear_ML'** in {target_name_display} **aktualisiert**.\")\n",
    "                    # 2. L√∂sche df_ML nach der finalen Zuweisung\n",
    "                    if 'df_ML' in globals():\n",
    "                        del globals()['df_ML']\n",
    "                        print(\"\\nüóëÔ∏è Der Arbeits-DataFrame **df_ML** wurde aus dem globalen Bereich entfernt (Aufgabe vollendet).\")\n",
    "                else:\n",
    "                    print(\"‚úÖ Analyse abgeschlossen. Die DataFrames wurden nicht ver√§ndert (df_ML bleibt ungel√∂scht).\")\n",
    "                assign_dd.disabled = True\n",
    "                b.disabled = True\n",
    "        self._add_step_widgets(\"8Ô∏è‚É£ Finale Speicherung & Bereinigung (L√∂scht df_ML):\",\n",
    "                               [Label(\"√úbertr√§gt die Cluster-Ergebnisse in das Ausgangs-DataFrame und l√∂scht den tempor√§ren Arbeits-DataFrame.\"), assign_dd],\n",
    "                               final_cleanup,\n",
    "                               \"OK (Finale Speicherung)\",\n",
    "                               btn_style='success')\n",
    "# F√ºhren Sie das interaktive Clustering-Tool aus\n",
    "ClusteringTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clustering-markdown-cell",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485346b4",
   "metadata": {},
   "source": [
    "1. Un√ºberwachtes Lernen (Unsupervised Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce7b563",
   "metadata": {},
   "source": [
    "4. Clustering\n",
    "Das Ziel des Clusterings war es, Kundensegmente mit √§hnlichen Eigenschaften zu identifizieren, ohne die loan_status-Information zu verwenden.\n",
    "\n",
    "Bedeutung: Mittels des KMeans-Algorithmus wurden die Kunden in 3 verschiedene Cluster (Gruppen) eingeteilt. Diese Cluster repr√§sentieren verschiedene Kundentypen basierend auf ihren finanziellen und demografischen Merkmalen.\n",
    "Erkenntnisse: Die resultierende cluster-Spalte wurde dem df_customer_data-DataFrame hinzugef√ºgt. Das Modell kann diese Information nun nutzen, um zu lernen, ob die Zugeh√∂rigkeit zu einem bestimmten Kundensegment die Wahrscheinlichkeit einer Kreditgenehmigung beeinflusst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a357f370",
   "metadata": {},
   "source": [
    "# Modell-Validierung und -Interpretation: \n",
    "- Sicherstellen, dass das Modell stabil ist und die Ergebnisse transparent sind (wichtig f√ºr die Erl√§uterung der Entscheidung an den Kunden und die Aufsichtsbeh√∂rden). Hierbei soll auch die Datenbank zur Vergleichbarkeit der Herkunft der Entscheidung (wie in Ihren Anweisungen gefordert) gespeicherte werden, basierend auf dem DataFrame df_ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdcd875",
   "metadata": {},
   "source": [
    "2. √úberwachtes Lernen (Supervised Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3Ô∏è‚É£ - Spaltenbereinigung\n",
    "cols_to_drop = []\n",
    "\n",
    "# Entfernen aller Spalten, die mit 'TEMP_Clear_ML' beginnen\n",
    "is_na_cols = [col for col in df_customer_data.columns if col.startswith('TEMP_Clear_ML')]\n",
    "cols_to_drop.extend(is_na_cols)\n",
    "\n",
    "# Entfernen der identifizierten Spalten\n",
    "df_customer_data = df_customer_data.drop(columns=list(set(cols_to_drop)), errors='ignore')\n",
    "\n",
    "df_customer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "# Muster: Cluster-Profilierung (Berechnung der Mittelwerte pro Cluster_ML) \n",
    "df_ML = df_customer_data.copy()\n",
    "\n",
    "# Liste der Features, die f√ºr das Profiling verwendet werden\n",
    "profiling_features = [col for col in df_ML.columns if col not in ['loan_id', 'loan_status', 'Cluster_ML']]\n",
    "\n",
    "# Berechne die Mittelwerte der Features und des Kreditstatus pro Cluster_ML\n",
    "# .agg() wird verwendet, um sowohl Mittelwerte f√ºr Features als auch die Zielvariable zu erhalten\n",
    "cluster_summary = df_ML.groupby('Cluster_ML')[profiling_features + ['loan_status']].mean()\n",
    "\n",
    "# F√ºge die Gr√∂√üe jedes Clusters hinzu\n",
    "cluster_size = df_ML['Cluster_ML'].value_counts().sort_index().rename('Count')\n",
    "cluster_summary = pd.concat([cluster_size, cluster_summary], axis=1)\n",
    "\n",
    "# Transponiere das DataFrame f√ºr eine bessere Lesbarkeit (Features als Zeilen, Cluster als Spalten)\n",
    "cluster_summary_T = cluster_summary.T\n",
    "\n",
    "# Runden der Werte\n",
    "cluster_summary_T = cluster_summary_T.round(3)\n",
    "\n",
    "print(\"Cluster-Profil (Mittelwerte der skalierten Features und Loan_Status pro Cluster):\")\n",
    "print(cluster_summary_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0316a",
   "metadata": {},
   "source": [
    "### Analyse und Interpretation (Vorschau)\n",
    "Profiling-Ergebnis cluster_summary_T direkt interpretieren.\n",
    "\n",
    "loan_status (Wichtigster Indikator): Schauen Sie sich die Zeile loan_status an. Der Wert repr√§sentiert den Anteil der bewilligten Kredite (1.0) in diesem Cluster. Ein Cluster mit einem hohen Wert (z.B. 0.85) ist eine 'Sicher-Segment', w√§hrend ein Cluster mit einem niedrigeren Wert (z.B. 0.35) ein 'Risiko-Segment' darstellt. Dies zeigt, wie gut das un√ºberwachte Clustering die √ºberwachte Zielvariable trennt.\n",
    "\n",
    "Skalierte Features: Bei skalierten Features bedeutet:\n",
    "\n",
    "Positiver Wert (+): Der Cluster-Mittelwert liegt √ºber dem Gesamtdurchschnitt aller Antr√§ge.\n",
    "\n",
    "Negativer Wert (-): Der Cluster-Mittelwert liegt unter dem Gesamtdurchschnitt aller Antr√§ge.\n",
    "\n",
    "Bin√§re Features (z.B. married_yes): Der Wert (z.B. 0.75) repr√§sentiert den Anteil der Bejahungen (True/Yes) in diesem Cluster.\n",
    "\n",
    "Anhand dieser Werte k√∂nnen Sie jedem Cluster einen aussagekr√§ftigen Namen geben, z.B. \"Gut verdienende Antragsteller mit guter Kredithistorie\" oder \"Geringverdiener, meist ledig\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d69115",
   "metadata": {},
   "source": [
    "### Die Hypothese lautet: Die Cluster-Zugeh√∂rigkeit, die wir gerade anhand der Kundendaten generiert haben, sollte die Vorhersagegenauigkeit verbessern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b031537f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122677a811ab457b8a3f4a5fec6c49d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.SupervisedTool at 0x12f0b7a10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausf√ºhren von Code in der aktuellen Zelle oder einer vorherigen Zelle abgest√ºrzt. \n",
      "\u001b[1;31mBitte √ºberpr√ºfen Sie den Code in der/den Zelle(n), um eine m√∂gliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "# √úberwachtes Lernen Clustering-App mit Dropdown-Men√ºs und erweiterter Ellbogen-Einstellung\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Optionale Bibliotheken\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    has_xgb = True\n",
    "except ImportError:\n",
    "    has_xgb = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    has_lgbm = True\n",
    "except ImportError:\n",
    "    has_lgbm = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "    has_catboost = True\n",
    "except ImportError:\n",
    "    has_catboost = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class SupervisedApp:\n",
    "    def __init__(self, original_df, target_column, feature_columns, nan_treatment, sampling_rate):\n",
    "        self.original_df = original_df\n",
    "        self.target_column = target_column\n",
    "        self.feature_columns = feature_columns\n",
    "        self.nan_treatment = nan_treatment\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.le = None\n",
    "        self.preprocessor = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        print(\"==================================================\")\n",
    "        print(\"‚û° SCHRITT 1: DATENVORBEREITUNG\")\n",
    "\n",
    "        if self.sampling_rate < 1.0:\n",
    "            self.df = self.original_df.sample(frac=self.sampling_rate, random_state=42)\n",
    "            print(f\"Datensatz auf {len(self.df)} Zeilen reduziert ({self.sampling_rate * 100:.2f}%).\")\n",
    "        else:\n",
    "            self.df = self.original_df.copy()\n",
    "\n",
    "        rows_to_drop = []\n",
    "        for col, treatment in self.nan_treatment.items():\n",
    "            if col not in self.df.columns: continue\n",
    "            if self.df[col].isnull().sum() == 0: continue\n",
    "\n",
    "            if treatment in ['median', 'mean', 'mode']:\n",
    "                is_na_col_name = f'is_na_{col}'\n",
    "                self.df[is_na_col_name] = self.df[col].isnull().astype(int)\n",
    "                print(f\"  - Spalte '{is_na_col_name}' als Indikator f√ºr fehlende Werte in '{col}' hinzugef√ºgt.\")\n",
    "\n",
    "            if treatment == 'median' or treatment == 'mean':\n",
    "                if pd.api.types.is_numeric_dtype(self.df[col]):\n",
    "                    value = self.df[col].median() if treatment == 'median' else self.df[col].mean()\n",
    "                    self.df[col] = self.df[col].fillna(value)\n",
    "                    print(f\"  - Numerisch '{col}': NaNs mit **{treatment.upper()}** imputiert.\")\n",
    "            elif treatment == 'mode':\n",
    "                mode_val = self.df[col].mode()\n",
    "                if not mode_val.empty:\n",
    "                    self.df[col] = self.df[col].fillna(mode_val[0])\n",
    "                    print(f\"  - Kategorisch/Bin√§r '{col}': NaNs mit **MODUS ('{mode_val[0]}')** imputiert.\")\n",
    "            elif treatment == 'drop_row':\n",
    "                rows_to_drop.extend(self.df[self.df[col].isnull()].index.tolist())\n",
    "                print(f\"  - '{col}': Zeilen mit NaNs zur Entfernung markiert.\")\n",
    "        \n",
    "        self.df.drop(list(set(rows_to_drop)), inplace=True)\n",
    "\n",
    "        y = self.df[self.target_column]\n",
    "        X = self.df[self.feature_columns]\n",
    "\n",
    "        if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "            self.le = LabelEncoder()\n",
    "            y_encoded = self.le.fit_transform(y)\n",
    "            print(f\"\\n‚úÖ Zielvariable '{y.name}' wurde Label-kodiert.\")\n",
    "        else:\n",
    "            y_encoded = y\n",
    "\n",
    "        numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "        categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', StandardScaler(), numeric_features),\n",
    "                ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "            ], remainder='passthrough')\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded if pd.api.types.is_categorical_dtype(y) or len(np.unique(y_encoded)) > 2 else None\n",
    "        )\n",
    "\n",
    "        self.X_train = self.preprocessor.fit_transform(self.X_train)\n",
    "        self.X_test = self.preprocessor.transform(self.X_test)\n",
    "\n",
    "        print(f\"\\n‚úÖ Daten aufgeteilt und vorverarbeitet.\")\n",
    "        print(f\"  - X_train: {self.X_train.shape}\")\n",
    "        print(f\"  - X_test: {self.X_test.shape}\")\n",
    "        return True\n",
    "\n",
    "    def run_models(self, selected_models):\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"‚û° SCHRITT 2: MODELLE TRAINIEREN & BEWERTEN\")\n",
    "        models = {\n",
    "            \"Logistische Regression\": LogisticRegression(), \"SGD\": SGDClassifier(), \"Ridge\": RidgeClassifier(),\n",
    "            \"KNN\": KNeighborsClassifier(), \"Entscheidungsbaum\": DecisionTreeClassifier(),\n",
    "            \"Random Forest\": RandomForestClassifier(), \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "            \"AdaBoost\": AdaBoostClassifier(), \"Bagging\": BaggingClassifier(), \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "            \"SVM\": SVC(probability=True), \"Naive Bayes\": GaussianNB(), \"MLP\": MLPClassifier(),\n",
    "        }\n",
    "        if has_xgb: models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "        if has_lgbm: models[\"LightGBM\"] = LGBMClassifier()\n",
    "        if has_catboost: models[\"CatBoost\"] = CatBoostClassifier(verbose=0)\n",
    "\n",
    "        for name in selected_models:\n",
    "            if name not in models: continue\n",
    "            model = models[name]\n",
    "            print(f\"Trainiere {name}...\")\n",
    "            try:\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                y_pred = model.predict(self.X_test)\n",
    "                y_pred_proba = model.predict_proba(self.X_test) if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "                self.results[name] = {\n",
    "                    'model': model,\n",
    "                    'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                    'precision': precision_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'recall': recall_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'f1_score': f1_score(self.y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'roc_auc': roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr') if y_pred_proba is not None and len(np.unique(self.y_test)) > 2 else (roc_auc_score(self.y_test, y_pred_proba[:, 1]) if y_pred_proba is not None else np.nan)\n",
    "                }\n",
    "                print(f\"‚úÖ {name} erfolgreich trainiert.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Fehler beim Trainieren von {name}: {e}\")\n",
    "\n",
    "    def display_results(self):\n",
    "        if not self.results: \n",
    "            print(\"Keine Ergebnisse zum Anzeigen.\")\n",
    "            return\n",
    "\n",
    "        results_df = pd.DataFrame(self.results).T.drop(columns=['model'])\n",
    "        results_df = results_df.astype(float).round(3)\n",
    "        results_df = results_df.sort_values(by=\"f1_score\", ascending=False)\n",
    "\n",
    "        if results_df.empty:\n",
    "            print(\"\\n‚ùå Keine Modelle konnten erfolgreich trainiert werden.\")\n",
    "            self.best_model = None\n",
    "            return\n",
    "\n",
    "        self.best_model_name = results_df.index[0]\n",
    "        self.best_model = self.results[self.best_model_name]['model']\n",
    "\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"‚û° SCHRITT 3: ERGEBNISSE\")\n",
    "        print(\"\\nüèÜ Modellvergleich (sortiert nach F1-Score):\")\n",
    "        display(results_df)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        results_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(kind='bar', ax=ax[0])\n",
    "        ax[0].set_title('Modell-Metriken')\n",
    "        ax[0].set_ylabel('Score')\n",
    "        ax[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        y_pred_best = self.best_model.predict(self.X_test)\n",
    "        cm = confusion_matrix(self.y_test, y_pred_best)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=self.le.classes_ if self.le else None)\n",
    "        disp.plot(ax=ax[1], cmap=\"Blues\")\n",
    "        ax[1].set_title(f'Konfusionsmatrix f√ºr {self.best_model_name}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        self.plot_feature_importance()\n",
    "\n",
    "    def plot_feature_importance(self):\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"‚û° SCHRITT 4: MERKMALSWICHTIGKEIT\")\n",
    "        if not hasattr(self.best_model, 'feature_importances_') and not hasattr(self.best_model, 'coef_'):\n",
    "            print(\"\\nHinweis: Das beste Modell unterst√ºtzt keine direkte Merkmalswichtigkeit.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            numeric_features = self.preprocessor.transformers_[0][2]\n",
    "            categorical_features = self.preprocessor.transformers_[1][2]\n",
    "            one_hot_encoder = self.preprocessor.named_transformers_['cat']\n",
    "            one_hot_feature_names = one_hot_encoder.get_feature_names_out(categorical_features)\n",
    "            feature_names = np.concatenate([numeric_features, one_hot_feature_names])\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Abrufen der Merkmalsnamen: {e}\")\n",
    "            return\n",
    "\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            importances = self.best_model.feature_importances_\n",
    "        else:\n",
    "            if self.best_model.coef_.ndim > 1:\n",
    "                importances = np.mean(np.abs(self.best_model.coef_), axis=0)\n",
    "            else:\n",
    "                importances = np.abs(self.best_model.coef_[0])\n",
    "\n",
    "        if len(importances) != len(feature_names):\n",
    "            print(f\"Warnung: Anzahl der Wichtigkeiten ({len(importances)}) stimmt nicht mit der Anzahl der Merkmale ({len(feature_names)}) √ºberein.\")\n",
    "            return\n",
    "\n",
    "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False).head(20)\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "        plt.title(f'Top 20 Merkmalswichtigkeiten f√ºr {self.best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_pca_visualization(self):\n",
    "        print(\"\\n==================================================\")\n",
    "        print(\"‚û° SCHRITT 5: PCA-VISUALISIERUNG\")\n",
    "        try:\n",
    "            pca = PCA(n_components=2)\n",
    "            X_test_pca = pca.fit_transform(self.X_test.toarray() if not isinstance(self.X_test, np.ndarray) else self.X_test)\n",
    "            y_pred_best = self.best_model.predict(self.X_test)\n",
    "            pca_df = pd.DataFrame(data=X_test_pca, columns=['PC1', 'PC2'])\n",
    "            \n",
    "            if self.le:\n",
    "                pca_df['True Label'] = self.le.inverse_transform(self.y_test)\n",
    "                pca_df['Predicted Label'] = self.le.inverse_transform(y_pred_best)\n",
    "            else:\n",
    "                pca_df['True Label'] = self.y_test\n",
    "                pca_df['Predicted Label'] = y_pred_best\n",
    "\n",
    "            pca_df['Correct'] = (pca_df['True Label'] == pca_df['Predicted Label'])\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.scatterplot(x='PC1', y='PC2', hue='True Label', data=pca_df, alpha=0.7, palette='viridis')\n",
    "            plt.title('PCA der Testdaten (Echte Labels)')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            correct_preds = pca_df[pca_df['Correct']]\n",
    "            incorrect_preds = pca_df[~pca_df['Correct']]\n",
    "            sns.scatterplot(x='PC1', y='PC2', hue='Predicted Label', data=correct_preds, alpha=0.5, palette='viridis', legend=True)\n",
    "            sns.scatterplot(x='PC1', y='PC2', data=incorrect_preds, color='red', marker='x', s=100, label='Fehlklassifiziert')\n",
    "            plt.title(f'PCA der Testdaten (Vorhersagen von {self.best_model_name})')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fehler bei der PCA-Visualisierung: {e}\")\n",
    "\n",
    "    def get_predictions_for_full_data(self):\n",
    "        X_full = self.original_df[self.feature_columns]\n",
    "        X_full_transformed = self.preprocessor.transform(X_full)\n",
    "        \n",
    "        predictions_dict = {}\n",
    "\n",
    "        raw_predictions = self.best_model.predict(X_full_transformed)\n",
    "        final_predictions = self.le.inverse_transform(raw_predictions) if self.le else raw_predictions\n",
    "        predictions_dict[\"Prediction\"] = final_predictions\n",
    "\n",
    "        if hasattr(self.best_model, \"predict_proba\"):\n",
    "            pred_proba = self.best_model.predict_proba(X_full_transformed)\n",
    "            if len(pred_proba.shape) > 1 and pred_proba.shape[1] > 1:\n",
    "                class_labels = self.le.classes_ if self.le else np.unique(self.original_df[self.target_column])\n",
    "                for i, class_label in enumerate(class_labels):\n",
    "                    predictions_dict[f\"Prob_{class_label}\"] = pred_proba[:, i]\n",
    "            else:\n",
    "                predictions_dict[\"Prediction_Probability\"] = pred_proba[:, 0] if pred_proba.ndim > 1 else pred_proba\n",
    "        \n",
    "        return predictions_dict\n",
    "\n",
    "class SupervisedTool:\n",
    "    def __init__(self):\n",
    "        self.available_dfs = {name: obj for name, obj in globals().items() if isinstance(obj, pd.DataFrame)}\n",
    "        self.selected_df_name = None\n",
    "        self.selected_df = None\n",
    "        self.target_column = None\n",
    "        self.feature_columns = None\n",
    "        self.nan_treatment = {}\n",
    "        self.sampling_rate = 1.0\n",
    "        self.app = None\n",
    "        self.all_steps_container = widgets.VBox([])\n",
    "        display(self.all_steps_container)\n",
    "        self.setup_df_selection()\n",
    "\n",
    "    def _add_step_widgets(self, title, widgets_list, handler, btn_label, btn_style=''):\n",
    "        all_widgets = [widgets.HTML(f\"<h3>{title}</h3>\")]\n",
    "        all_widgets.extend(widgets_list)\n",
    "        step_container = widgets.VBox(all_widgets)\n",
    "        btn = widgets.Button(description=btn_label, button_style=btn_style)\n",
    "        output_for_step = widgets.Output()\n",
    "        btn.on_click(lambda b: handler(b, output_for_step))\n",
    "        self.all_steps_container.children += (step_container, btn, output_for_step)\n",
    "        return btn, output_for_step\n",
    "\n",
    "    def setup_df_selection(self):\n",
    "        if not self.available_dfs:\n",
    "            self.all_steps_container.children = (widgets.HTML(\"‚ùå **Keine DataFrames gefunden.** Bitte laden Sie einen DataFrame in die Umgebung.\"),)\n",
    "            return\n",
    "        df_options = list(self.available_dfs.keys())\n",
    "        df_dd = widgets.Dropdown(options=df_options, description=\"DataFrame:\")\n",
    "\n",
    "        def on_ok_click(b, output_for_step):\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                self.selected_df_name = df_dd.value\n",
    "                self.selected_df = self.available_dfs[self.selected_df_name].copy()\n",
    "                print(f\"‚úÖ DataFrame **'{self.selected_df_name}'** ausgew√§hlt.\")\n",
    "            df_dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_target_selection()\n",
    "\n",
    "        self._add_step_widgets(\"1Ô∏è‚É£ DataFrame ausw√§hlen:\", [df_dd], on_ok_click, \"OK (DataFrame w√§hlen)\")\n",
    "\n",
    "    def setup_target_selection(self):\n",
    "        column_options = self.selected_df.columns.tolist()\n",
    "        target_dd = widgets.Dropdown(options=column_options, description=\"Zielvariable (y):\")\n",
    "\n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.target_column = target_dd.value\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚úÖ Zielvariable: **{self.target_column}**\")\n",
    "            target_dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_feature_selection()\n",
    "\n",
    "        self._add_step_widgets(\"2Ô∏è‚É£ Zielvariable (y) ausw√§hlen:\", [target_dd], on_next_click, \"Weiter zu Feature-Auswahl\")\n",
    "\n",
    "    def setup_feature_selection(self):\n",
    "        all_possible_features = self.selected_df.columns.drop(self.target_column).tolist()\n",
    "        \n",
    "        exclude_sm = widgets.SelectMultiple(\n",
    "            options=all_possible_features,\n",
    "            value=[],\n",
    "            description=\"Auszuschlie√üen:\",\n",
    "            rows=10,\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        def on_next_click(b, output_for_step):\n",
    "            excluded_cols = list(exclude_sm.value)\n",
    "            self.feature_columns = [col for col in all_possible_features if col not in excluded_cols]\n",
    "            \n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚úÖ {len(excluded_cols)} Spalten ausgeschlossen.\")\n",
    "                print(f\"‚úÖ {len(self.feature_columns)} Features f√ºr die Analyse ausgew√§hlt.\")\n",
    "\n",
    "            exclude_sm.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_sampling_selection()\n",
    "        \n",
    "        self._add_step_widgets(\n",
    "            \"3Ô∏è‚É£ Features (X) ausw√§hlen (optional Spalten ausschlie√üen):\",\n",
    "            [widgets.HTML(\"Standardm√§√üig werden alle Spalten au√üer der Zielvariable verwendet. W√§hlen Sie hier Spalten aus, die Sie **entfernen** m√∂chten.\"), exclude_sm],\n",
    "            on_next_click,\n",
    "            \"Weiter zu Sampling\"\n",
    "        )\n",
    "\n",
    "    def setup_sampling_selection(self):\n",
    "        n_rows = len(self.selected_df)\n",
    "        row_input = widgets.IntText(\n",
    "            value=n_rows, min=1, max=n_rows, step=1,\n",
    "            description='Max. Zeilen:', disabled=False, style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        def on_next_click(b, output_for_step):\n",
    "            target_rows = row_input.value\n",
    "            if target_rows < 1 or target_rows > n_rows:\n",
    "                with output_for_step:\n",
    "                    clear_output(wait=True)\n",
    "                    print(f\"‚ùå FEHLER: Ung√ºltige Zeilenzahl. W√§hlen Sie zwischen 1 und {n_rows}.\")\n",
    "                return\n",
    "            self.sampling_rate = target_rows / n_rows\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                if self.sampling_rate < 1.0:\n",
    "                    print(f\"‚úÖ Sampling: **{target_rows} Zeilen** ausgew√§hlt ({self.sampling_rate*100:.2f}%).\")\n",
    "                else:\n",
    "                    print(\"‚úÖ Sampling: **Voller Datensatz** ausgew√§hlt.\")\n",
    "            row_input.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_nan_treatment_selection()\n",
    "\n",
    "        self._add_step_widgets(\n",
    "            \"4Ô∏è‚É£ Datenreduzierung (Sampling) ausw√§hlen:\",\n",
    "            [widgets.Label(f\"Geben Sie die maximale Anzahl der zu analysierenden Zeilen ein (Aktuelle Gr√∂√üe: {n_rows}).\"), row_input],\n",
    "            on_next_click,\n",
    "            \"Weiter zu Fehlerbehandlung\"\n",
    "        )\n",
    "\n",
    "    def setup_nan_treatment_selection(self):\n",
    "        # Pass only potential feature columns to check for NaNs\n",
    "        df_check = self.selected_df.drop(columns=[self.target_column])\n",
    "        nan_cols = df_check.columns[df_check.isnull().any()].tolist()\n",
    "        self.treatment_widgets = {}\n",
    "        widget_list = []\n",
    "        \n",
    "        num_options = {\"Median (Empfohlen)\": 'median', \"Mittelwert (Mean)\": 'mean', \"Zeilen entfernen\": 'drop_row'}\n",
    "        cat_options = {\"Modus (Mode/H√§ufigster Wert)\": 'mode', \"Zeilen entfernen\": 'drop_row'}\n",
    "\n",
    "        if nan_cols:\n",
    "            for col in nan_cols:\n",
    "                col_dtype = df_check[col].dtype\n",
    "                if np.issubdtype(col_dtype, np.number):\n",
    "                    options, default_value = num_options, 'median'\n",
    "                    label = f\"üî¢ **{col}** (Numerisch, {df_check[col].isnull().sum()} NaNs):\"\n",
    "                else:\n",
    "                    options, default_value = cat_options, 'mode'\n",
    "                    label = f\"üî† **{col}** (Kategorisch, {df_check[col].isnull().sum()} NaNs):\"\n",
    "                \n",
    "                dropdown = widgets.Dropdown(options=options, value=default_value, description=\"Methode:\")\n",
    "                self.treatment_widgets[col] = dropdown\n",
    "                widget_list.append(widgets.VBox([widgets.Label(label), dropdown]))\n",
    "        else:\n",
    "            widget_list.append(widgets.HTML(\"‚úÖ **Keine fehlenden Werte** in den Features gefunden.\"))\n",
    "\n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.nan_treatment = {col: dd.value for col, dd in self.treatment_widgets.items()}\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                print(\"‚úÖ NaN-Behandlungen gespeichert.\")\n",
    "            for dd in self.treatment_widgets.values(): dd.disabled = True\n",
    "            b.disabled = True\n",
    "            self.setup_model_selection()\n",
    "\n",
    "        self._add_step_widgets(\n",
    "            \"5Ô∏è‚É£ Behandlung fehlender Werte pro Spalte ausw√§hlen:\",\n",
    "            widget_list, on_next_click, \"Weiter zu Modellauswahl\"\n",
    "        )\n",
    "\n",
    "    def setup_model_selection(self):\n",
    "        models = {\n",
    "            \"Logistische Regression\": LogisticRegression(), \"SGD\": SGDClassifier(), \"Ridge\": RidgeClassifier(),\n",
    "            \"KNN\": KNeighborsClassifier(), \"Entscheidungsbaum\": DecisionTreeClassifier(),\n",
    "            \"Random Forest\": RandomForestClassifier(), \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "            \"AdaBoost\": AdaBoostClassifier(), \"Bagging\": BaggingClassifier(), \"ExtraTrees\": ExtraTreesClassifier(),\n",
    "            \"SVM\": SVC(probability=True), \"Naive Bayes\": GaussianNB(), \"MLP\": MLPClassifier(),\n",
    "        }\n",
    "        if has_xgb: models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "        if has_lgbm: models[\"LightGBM\"] = LGBMClassifier()\n",
    "        if has_catboost: models[\"CatBoost\"] = CatBoostClassifier(verbose=0)\n",
    "\n",
    "        self.model_selector = widgets.SelectMultiple(\n",
    "            options=list(models.keys()), value=list(models.keys()),\n",
    "            description=\"Modelle:\", rows=len(models)\n",
    "        )\n",
    "\n",
    "        def on_next_click(b, output_for_step):\n",
    "            self.selected_models = self.model_selector.value\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚úÖ {len(self.selected_models)} Modelle ausgew√§hlt.\")\n",
    "            self.model_selector.disabled = True\n",
    "            b.disabled = True\n",
    "            self.run_analysis()\n",
    "\n",
    "        self._add_step_widgets(\n",
    "            \"6Ô∏è‚É£ Modelle f√ºr den Vergleich ausw√§hlen:\",\n",
    "            [self.model_selector], on_next_click, \"Analyse starten\"\n",
    "        )\n",
    "\n",
    "    def run_analysis(self):\n",
    "        self.app = SupervisedApp(\n",
    "            original_df=self.selected_df,\n",
    "            target_column=self.target_column,\n",
    "            feature_columns=self.feature_columns,\n",
    "            nan_treatment=self.nan_treatment,\n",
    "            sampling_rate=self.sampling_rate\n",
    "        )\n",
    "        if self.app.prepare_data():\n",
    "            self.app.run_models(self.selected_models)\n",
    "            self.app.display_results()\n",
    "            \n",
    "            if self.app.best_model is not None:\n",
    "                self.app.plot_pca_visualization()\n",
    "                self.setup_final_assignment()\n",
    "            else:\n",
    "                print(\"\\n‚ùå Kein bestes Modell gefunden. Die Zuweisung von Vorhersagen wird √ºbersprungen.\")\n",
    "\n",
    "    def setup_final_assignment(self):\n",
    "        assignment_options = {\"Prediction\": \"Prediction\"}\n",
    "        if hasattr(self.app.best_model, \"predict_proba\"):\n",
    "            assignment_options[\"Prediction_Probability\"] = \"Prediction_Probability\"\n",
    "\n",
    "        assign_sm = widgets.SelectMultiple(\n",
    "            options=list(assignment_options.keys()), value=list(assignment_options.keys()),\n",
    "            description=\"Spalten hinzuf√ºgen:\", style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        def on_assign_click(b, output_for_step):\n",
    "            selected_options = assign_sm.value\n",
    "            with output_for_step:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                all_predictions = self.app.get_predictions_for_full_data()\n",
    "\n",
    "                if \"Prediction\" in selected_options:\n",
    "                    globals()[self.selected_df_name][\"Prediction\"] = all_predictions[\"Prediction\"]\n",
    "                    print(f\"‚úÖ Spalte 'Prediction' wurde zum DataFrame '{self.selected_df_name}' hinzugef√ºgt.\")\n",
    "\n",
    "                if \"Prediction_Probability\" in selected_options:\n",
    "                    for col_name, values in all_predictions.items():\n",
    "                        if col_name.startswith('Prob_') or col_name == \"Prediction_Probability\":\n",
    "                            globals()[self.selected_df_name][col_name] = values\n",
    "                            print(f\"‚úÖ Spalte '{col_name}' wurde zum DataFrame '{self.selected_df_name}' hinzugef√ºgt.\")\n",
    "            \n",
    "            b.disabled = True\n",
    "\n",
    "        self._add_step_widgets(\n",
    "            \"7Ô∏è‚É£ Vorhersagen zum DataFrame hinzuf√ºgen:\",\n",
    "            [assign_sm], on_assign_click, \"Zuweisen\"\n",
    "        )\n",
    "\n",
    "# Start the tool\n",
    "SupervisedTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-building-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Modelltraining Random Forest Classifier/Logistische Regression\n",
    "# ============================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Definiere Features (X) und Target (y)\n",
    "X = df_customer_data.drop(['loan_status', 'loan_id'], axis=1)\n",
    "y = df_customer_data['loan_status']\n",
    "\n",
    "# Aufteilen der Daten in Trainings- und Testsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Logistische Regression\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print(log_reg.coef_)\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rand_forest = RandomForestClassifier(random_state=42)\n",
    "rand_forest.fit(X_train, y_train)\n",
    "y_pred_rand_forest = rand_forest.predict(X_test)\n",
    "\n",
    "# Evaluierung\n",
    "print('--- Logistische Regression ---')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_log_reg):.4f}')\n",
    "print(classification_report(y_test, y_pred_log_reg))\n",
    "\n",
    "print('--- Random Forest Classifier ---')\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_rand_forest):.4f}')\n",
    "print(classification_report(y_test, y_pred_rand_forest))\n",
    "\n",
    "# df_ML erstellen\n",
    "df_ML = X_test.copy()\n",
    "df_ML['loan_status_actual'] = y_test\n",
    "df_ML['loan_status_pred_log_reg'] = y_pred_log_reg\n",
    "df_ML['loan_status_pred_rand_forest'] = y_pred_rand_forest\n",
    "\n",
    "print('df_ML DataFrame erstellt.')\n",
    "df_ML.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b67103",
   "metadata": {},
   "source": [
    "# Der Random Forest Classifier \n",
    "- ist der eindeutige Gewinner und sollte f√ºr die Bankaufgabe zur Vorhersage des Kreditstatus verwendet werden.\n",
    "    - H√∂chste Gesamtgenauigkeit: Mit 87.85% Accuracy ist er dem linearen Modell (Logistische Regression) √ºberlegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e25fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance des Random Forest Classifiers\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Extrahiere die Feature Importances\n",
    "importances = rand_forest.feature_importances_\n",
    "\n",
    "# 2. Ordne die Wichtigkeiten den Feature-Namen zu\n",
    "feature_names = X_train.columns\n",
    "feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "# 3. Sortiere die Features nach Wichtigkeit (absteigend)\n",
    "sorted_importances = feature_importances.sort_values(ascending=False)\n",
    "\n",
    "# 4. Erstellung des Plots\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.barplot(x=sorted_importances.values, y=sorted_importances.index, palette='viridis')\n",
    "\n",
    "plt.title('Feature Importance (Merkmalsbedeutung) des Random Forest Classifiers')\n",
    "plt.xlabel('Wichtigkeit (Gini Importance)')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('random_forest_feature_importance.png')\n",
    "plt.show()\n",
    "\n",
    "# 5. Ausgabe der Top 5 wichtigsten Features\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Top 5 wichtigsten Features f√ºr die Kreditentscheidung:\")\n",
    "print(sorted_importances.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51654d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot zur darstellung von loghistische regresion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd \n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ANNAHME: X und y sind die vorbereiteten Features und das Ziel-Target\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if 'credit_history' in X.columns and 'loan_to_income_ratio' in X.columns:\n",
    "    feature_1 = 'credit_history'\n",
    "    feature_2 = 'loan_to_income_ratio'\n",
    "    \n",
    "    # Reduziere die Daten nur auf diese zwei Features\n",
    "    X_plot = X[[feature_1, feature_2]].copy()\n",
    "    y_plot = y.copy()\n",
    "\n",
    "    \n",
    "    # Daten f√ºr die Darstellung skalieren\n",
    "    scaler = StandardScaler()\n",
    "    X_plot_scaled = scaler.fit_transform(X_plot)\n",
    "    \n",
    "    X_train_plot, X_test_plot, y_train_plot, y_test_plot = train_test_split(\n",
    "        X_plot_scaled, y_plot, test_size=0.2, random_state=42, stratify=y_plot\n",
    "    )\n",
    "    \n",
    "    # Erneutes Training der Logistischen Regression nur mit diesen 2 Features\n",
    "    log_reg_2d = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    log_reg_2d.fit(X_train_plot, y_train_plot)\n",
    "    \n",
    "    # Erstellen des Grids f√ºr die Entscheidungsgrenze\n",
    "    h = 0.02  # Schrittweite im Mesh\n",
    "    x_min, x_max = X_plot_scaled[:, 0].min() - 0.5, X_plot_scaled[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_plot_scaled[:, 1].min() - 0.5, X_plot_scaled[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Vorhersage der Wahrscheinlichkeiten auf dem gesamten Grid\n",
    "    Z = log_reg_2d.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plotten der Entscheidungsgrenzen (Konturlinien)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # 1. Kontur-Plot (Wahrscheinlichkeitsfl√§che)\n",
    "    contour = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu, alpha=0.6, levels=np.linspace(0, 1, 15))\n",
    "    cbar = plt.colorbar(contour)\n",
    "    cbar.set_label('Vorhersagte Wahrscheinlichkeit (P(y=1))')\n",
    "    \n",
    "    # 2. Scatter Plot der tats√§chlichen Datenpunkte\n",
    "    sns.scatterplot(\n",
    "        x=X_train_plot[:, 0], \n",
    "        y=X_train_plot[:, 1], \n",
    "        hue=y_train_plot,\n",
    "        palette={0: 'red', 1: 'green'}, \n",
    "        style=y_train_plot,\n",
    "        markers=['o', 's'], \n",
    "        s=80,\n",
    "        edgecolor='k'\n",
    "        # label=y_plot.name wurde entfernt!\n",
    "    )\n",
    "    \n",
    "    # 3. Entscheidungsgrenze (dort, wo P(y=1) = 0.5 ist)\n",
    "    plt.contour(xx, yy, Z, colors='k', linestyles='--', levels=[0.5], linewidths=2)\n",
    "    \n",
    "    plt.title('Entscheidungsgrenze der Logistischen Regression (2 Features)')\n",
    "    plt.xlabel(f'{feature_1} (skaliert)')\n",
    "    plt.ylabel(f'{feature_2} (skaliert)')\n",
    "    # Die Legende wird hier korrekt mit dem Titel aus dem y_plot.name gesetzt\n",
    "    plt.legend(title=y_plot.name)\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Muster konnte nicht ausgef√ºhrt werden: 'total_income' oder 'loan_amount' nicht in den Features gefunden.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b8842",
   "metadata": {},
   "source": [
    "- Blaues Feld (Links oben) : \n",
    "    - Das Modell sagt mit hoher Sicherheit voraus, dass die loan_status 0.0 (z.B. Kredit nicht gew√§hrt oder kein Ausfall) sein wird.\n",
    "- Rotes/Oranges Feld (Rechts unten): \n",
    "    - Das Modell sagt mit hoher Sicherheit voraus, dass die loan_status 1.0 (z.B. Kredit gew√§hrt oder Ausfall) sein wird.\n",
    "- Schwarze gestrichelte Linie :\n",
    "    - Diese Linie trennt die Bereiche. Auf der Linie ist die Vorhersagewahrscheinlichkeit exakt 50% (0.5)\n",
    "- Gr√ºne Quadrate (Marker üü©)\n",
    "    - Diese Punkte repr√§sentieren reale F√§lle im Trainingsdatensatz, bei denen der loan_status tats√§chlich 1.0 war (die positive Klasse).\n",
    "- Rote Kreise (Marker üî¥)\n",
    "    - Diese Punkte repr√§sentieren reale F√§lle im Trainingsdatensatz, bei denen der loan_status tats√§chlich 0.0 war (die negative Klasse).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732496f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gewichtung der Kreditvergabe \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "\n",
    "# ==============================================================================\n",
    "# ANNAHME: X und y sind die vorbereiteten Features und das Ziel-Target (y=loan_status)\n",
    "#         log_reg ist Ihr trainiertes Logistische-Regression-Modell\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. √úberpr√ºfen, ob das Modell existiert\n",
    "if 'log_reg' not in locals() or not isinstance(log_reg, LogisticRegression):\n",
    "    # FALLS NICHT TRAINIERT: Logistische Regression (erneut) trainieren\n",
    "    # Da wir uns im ML-Kontext befinden, gehen wir davon aus, dass X und y existieren.\n",
    "    \n",
    "    # Sicherstellen, dass die Daten f√ºr die Koeffizientenanalyse skaliert sind\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Identifiziere die numerischen Spalten (die skaliert werden m√ºssen)\n",
    "    numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    \n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "    X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    log_reg.fit(X_train_final, y_train_final)\n",
    "    X_final = X_train_final.columns\n",
    "else:\n",
    "    # FALLS BEREITS TRAINIERT: Verwende die Spalten des trainierten X_train\n",
    "    X_final = X_train.columns\n",
    "    # Sicherstellen, dass die Koeffizienten der vollen LogReg (log_reg) verwendet werden\n",
    "    \n",
    "# 2. Koeffizienten extrahieren\n",
    "coefficients = log_reg.coef_[0]\n",
    "feature_names = X_final\n",
    "\n",
    "# 3. DataFrame f√ºr die Koeffizienten erstellen\n",
    "df_coef = pd.DataFrame({\n",
    "    'Merkmal': feature_names,\n",
    "    'Koeffizient (Gewichtung)': coefficients\n",
    "})\n",
    "\n",
    "# 4. Interpretierbare Spalten hinzuf√ºgen\n",
    "# Der Koeffizient 'c' bedeutet, dass eine Erh√∂hung des Merkmals um 1 Standardabweichung\n",
    "# die Log-Odds f√ºr P(Zusage) um 'c' erh√∂ht.\n",
    "df_coef['Auswirkung'] = np.where(df_coef['Koeffizient (Gewichtung)'] > 0, 'Positiv (Zusage)', 'Negativ (Ablehnung)')\n",
    "df_coef['Absoluter Wert'] = df_coef['Koeffizient (Gewichtung)'].abs()\n",
    "\n",
    "# 5. Sortieren und Ausgeben der Top-Merkmale\n",
    "df_coef_sorted = df_coef.sort_values(by='Absoluter Wert', ascending=False)\n",
    "df_coef_sorted = df_coef_sorted.drop(columns='Absoluter Wert')\n",
    "\n",
    "print(\"!!! WICHTIGSTE MERKMALE F√úR DIE KREDITENTSCHEIDUNG (LOGISTISCHE REGRESSION) !!!\")\n",
    "print(\"---------------------------------------------------------------------------------\")\n",
    "print(\"Da die Daten skaliert sind, zeigt der Koeffizient, welche Merkmale die st√§rkste Auswirkung\")\n",
    "print(\"auf die Log-Odds der Kreditzusage haben.\")\n",
    "\n",
    "display(df_coef_sorted.head(10))\n",
    "\n",
    "# 6. Besondere Betrachtung der Einkommens- und Kredit-Merkmale (wenn vorhanden)\n",
    "print(\"\\n--- Fokus auf Einkommen und Kredit ---\")\n",
    "focus_cols = ['income', 'loan_amount', 'ratio']\n",
    "df_focus = df_coef_sorted[df_coef_sorted['Merkmal'].str.contains('|'.join(focus_cols), case=False, na=False)]\n",
    "display(df_focus)\n",
    "\n",
    "# 7. Interpretation des Verh√§ltnisses (Kredit zu Einkommen)\n",
    "# Die Logistische Regression kombiniert die Koeffizienten von 'total_income' und 'loan_amount'\n",
    "# Beachten Sie, dass in Ihrem Plot 'loan_to_income_ratio' bereits als Feature enthalten war,\n",
    "# was die Komplexit√§t reduziert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7b3c5",
   "metadata": {},
   "source": [
    "# Klassifikation:\n",
    "\n",
    "- Alle Punkte rechts und oberhalb dieser Linie werden vom Modell als angenommener Kredit (1.0) vorhergesagt.\n",
    "\n",
    "- Alle Punkte links und unterhalb dieser Linie werden vom Modell als abgelehnter Kredit (0.0) vorhergesagt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372ce96",
   "metadata": {},
   "source": [
    "5. Modelltraining und Evaluierung (df_ML)\n",
    "In diesem letzten Schritt wurden zwei verschiedene Klassifikationsmodelle trainiert und bewertet, um vorherzusagen, ob ein Kreditantrag genehmigt wird.\n",
    "\n",
    "Modelle:\n",
    "\n",
    "Logistische Regression: Ein einfaches, gut interpretierbares Modell, das oft als Basismodell verwendet wird.\n",
    "Random Forest Classifier: Ein komplexeres Ensemble-Modell, das in der Regel eine h√∂here Vorhersagegenauigkeit erzielt.\n",
    "Ergebnisse:\n",
    "\n",
    "Beide Modelle wurden auf ihre Accuracy (Genauigkeit) und mit einem Classification Report (der Pr√§zision, Recall und F1-Score zeigt) bewertet.\n",
    "Der Random Forest Classifier zeigte eine h√∂here Genauigkeit, was darauf hindeutet, dass die komplexeren Muster, die dieses Modell erkennen kann, in den Daten vorhanden sind.\n",
    "df_ML DataFrame:\n",
    "\n",
    "Dieser DataFrame wurde erstellt, um die Vorhersagen der Modelle mit den tats√§chlichen Werten zu vergleichen. Er enth√§lt die urspr√ºnglichen Testdaten, die tats√§chliche Kreditentscheidung (loan_status_actual) und die Vorhersagen beider Modelle. Dies erm√∂glicht eine detaillierte Analyse der Modellleistung und die Identifizierung von F√§llen, in denen die Modelle Fehler gemacht haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-plot-markdown-cell",
   "metadata": {},
   "source": [
    "# Cluster-Analyse: Polar Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0442",
   "metadata": {},
   "source": [
    "###  Die Merkmale im Radar Plot, die direkt das Risiko beeinflussen:\n",
    "\n",
    "credit_history (Kredit-Historie): Ein hoher Wert (weit au√üen, nahe 10) bedeutet eine gute Historie, d.h. niedriges Risiko.\n",
    "\n",
    "loan_to_income_ratio (Kredit-Einkommen-Verh√§ltnis): Ein hoher Wert (weit au√üen) bedeutet, der Kredit ist im Verh√§ltnis zum Einkommen sehr hoch, d.h. hohes Risiko."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-plot-code-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polar Plot erstellen mit sortierung der darstellung \n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# I. DATENVORBEREITUNG F√úR ANALYSE (Minimal)\n",
    "df_ML = df_customer_data.copy()\n",
    "\n",
    "# 1. Sicherstellen, dass Cluster_ML als Integer vorliegt (f√ºr Mittelwertberechnung)\n",
    "if 'Cluster_ML' in df_ML.columns:\n",
    "    try:\n",
    "        df_ML['Cluster_ML'] = df_ML['Cluster_ML'].astype(int)\n",
    "    except:\n",
    "        print(\"WARNUNG: 'Cluster_ML' konnte nicht in Integer konvertiert werden. √úberpr√ºfen Sie die Daten.\")\n",
    "else:\n",
    "    print(\"FEHLER: 'Cluster_ML' Spalte nicht im DataFrame gefunden. Analyse nicht m√∂glich.\")\n",
    "\n",
    "# 2. Feature Engineering (muss nur dann laufen, wenn diese Features nicht existieren)\n",
    "if 'applicant_income' in df_ML.columns and 'coapplicant_income' in df_ML.columns and 'total_income' not in df_ML.columns:\n",
    "    df_ML['total_income'] = df_ML['applicant_income'] + df_ML['coapplicant_income']\n",
    "    if 'loan_amount' in df_ML.columns:\n",
    "        df_ML['loan_to_income_ratio'] = df_ML['loan_amount'] / np.where(df_ML['total_income'] == 0, np.nan, df_ML['total_income'])\n",
    "    \n",
    "    # Imputation der erstellten NaN-Werte (optional, aber empfohlen f√ºr Mittelwerte)\n",
    "    if 'loan_to_income_ratio' in df_ML.columns:\n",
    "        df_ML['loan_to_income_ratio'].fillna(df_ML['loan_to_income_ratio'].median(), inplace=True)\n",
    "\n",
    "# II. CLUSTER-ANALYSE UND VISUALISIERUNG\n",
    "COLOR_MAP_CLUSTER_HEX = {\n",
    "    0: '#1f77b4',   # Blau\n",
    "    1: '#ff7f0e',   # Orange\n",
    "    2: '#2ca02c',   # Gr√ºn\n",
    "    3: '#d62728',   # Rot\n",
    "}\n",
    "\n",
    "# Farb-Map f√ºr Plotly (RGB-Strings)\n",
    "COLOR_MAP_CLUSTER_PLOTLY = {\n",
    "    0: 'rgb(31, 119, 180)',   \n",
    "    1: 'rgb(255, 127, 14)',   \n",
    "    2: 'rgb(44, 160, 44)',    \n",
    "    3: 'rgb(214, 39, 40)',    \n",
    "}\n",
    "\n",
    "# String-basierte Map F√úR SEABORN (Hex-Codes)\n",
    "COLOR_DISCRETE_MAP_SEABORN = {str(k): v for k, v in COLOR_MAP_CLUSTER_HEX.items()}\n",
    "\n",
    "# Manuelle Zuweisung von Cluster_ML-Labels (passen Sie diese an Ihre Daten an)\n",
    "cluster_to_label_map = {\n",
    "    0: 'Cluster 0: Niedriges Einkommen',\n",
    "    1: 'Cluster 1: Hohes Einkommen',\n",
    "    2: 'Cluster 2: Mittleres Risiko'\n",
    "}\n",
    "# Radarplot-Funktion\n",
    "def erstelle_cluster_radar_plot(df_raw, grouping_col_name, exclude_cols, r_label, title_format):\n",
    "    df_processed = df_raw.copy()\n",
    "    \n",
    "    # Filtern aller numerischen Spalten, die nicht ausgeschlossen sind\n",
    "    metric_cols = [\n",
    "        c for c in df_processed.columns \n",
    "        if c not in exclude_cols and pd.api.types.is_numeric_dtype(df_processed[c]) and c != grouping_col_name\n",
    "    ]\n",
    "    \n",
    "    grouped_means = df_processed.groupby(grouping_col_name)[metric_cols].mean()\n",
    "\n",
    "    # 1. Logische Gruppierung der Metriken\n",
    "    financial_metrics = [c for c in metric_cols if any(word in c.lower() for word in ['income', 'loan_amount', 'term', 'ratio'])]\n",
    "    risk_metrics = [c for c in metric_cols if 'credit_history' in c.lower()]\n",
    "    demographic_metrics = [c for c in metric_cols if c not in financial_metrics and c not in risk_metrics]\n",
    "    \n",
    "    # 2. Sortierung INNERHALB der Gruppen nach Variabilit√§t\n",
    "    def sort_metrics_by_variance(metrics):\n",
    "        if not metrics: return []\n",
    "        std_devs = grouped_means[metrics].std(axis=0).sort_values(ascending=False)\n",
    "        return std_devs.index.tolist()\n",
    "\n",
    "    # Erstelle die finale, geordnete Liste der Metriken\n",
    "    sorted_metrics = (\n",
    "        sort_metrics_by_variance(financial_metrics) + \n",
    "        sort_metrics_by_variance(risk_metrics) +\n",
    "        sort_metrics_by_variance(demographic_metrics)\n",
    "    )\n",
    "    \n",
    "    grouped_means = grouped_means[sorted_metrics]\n",
    "\n",
    "    # Skalierung auf 0‚Äì10 f√ºr den Plot (MinMaxScaler auf Cluster-Mittelwerten)\n",
    "    scaler = MinMaxScaler()\n",
    "    df_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(grouped_means),\n",
    "        index=grouped_means.index,\n",
    "        columns=grouped_means.columns\n",
    "    ) * 10\n",
    "    df_normalized[grouping_col_name] = df_normalized.index\n",
    "\n",
    "    # Long-Format f√ºr Plotly\n",
    "    df_plot = df_normalized.melt(\n",
    "        id_vars=grouping_col_name,\n",
    "        var_name=\"Metrik\",\n",
    "        value_name=r_label\n",
    "    )\n",
    "    df_plot['Cluster_Label'] = df_plot[grouping_col_name].map(cluster_to_label_map)\n",
    "    \n",
    "    # Radarplot\n",
    "    fig = px.line_polar(\n",
    "        df_plot,\n",
    "        r=r_label,\n",
    "        theta=\"Metrik\",\n",
    "        color=\"Cluster_Label\", \n",
    "        line_close=True,\n",
    "        title=title_format.format(group_col=grouping_col_name),\n",
    "        color_discrete_map={cluster_to_label_map[k]: COLOR_MAP_CLUSTER_PLOTLY[k] for k in cluster_to_label_map if k in df_processed[grouping_col_name].unique()}\n",
    "    )\n",
    "    fig.update_traces(fill=\"toself\")\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, 10])),\n",
    "        height=700,\n",
    "        title_font_size=20\n",
    "    )\n",
    "    display(fig)\n",
    "\n",
    "df_final = df_ML.copy()\n",
    "df_final[\"Cluster_ML_str\"] = df_final[\"Cluster_ML\"].astype(str) \n",
    "cluster_to_label_map_str = {str(k): v for k, v in cluster_to_label_map.items()}\n",
    "df_final[\"cluster_label\"] = df_final[\"Cluster_ML_str\"].map(cluster_to_label_map_str)\n",
    "\n",
    "\n",
    "# STATISTISCHE ZUORDNUNGSTABELLE (Mittelwerte pro Cluster - UNISKALIERT)\n",
    "numerical_features_unscaled = [col for col in df_ML.columns if pd.api.types.is_numeric_dtype(df_ML[col]) and col not in ['Cluster_ML'] and not col.startswith('is_na_')]\n",
    "\n",
    "cluster_profiling = df_ML.groupby(\"Cluster_ML\")[numerical_features_unscaled].mean().reset_index()\n",
    "cluster_profiling[\"cluster_label\"] = cluster_profiling[\"Cluster_ML\"].map(cluster_to_label_map)\n",
    "\n",
    "print(\"!!! STATISTISCHE ZUORDNUNGSTABELLE (Mittelwerte pro Cluster - unskaliert) !!!\")\n",
    "cluster_profiling = cluster_profiling[['cluster_label'] + [col for col in cluster_profiling.columns if col not in ['Cluster_ML', 'cluster_label']]]\n",
    "print(cluster_profiling.to_string(index=False, float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "# KORRIGIERTER RADARPLOT\n",
    "plt.figure(figsize=(4, 3))\n",
    "erstelle_cluster_radar_plot(\n",
    "    df_raw=df_ML,\n",
    "    grouping_col_name=\"Cluster_ML\",\n",
    "    exclude_cols=[\"loan_id\", \"cluster_label\"] + [c for c in df_ML.columns if c.endswith(('_yes', '_graduate', '_semiurban'))], \n",
    "    r_label=\"Wert (0-10)\",\n",
    "    title_format=\"Radar Plot der Cluster-Merkmale ({group_col})\"\n",
    ")\n",
    "\n",
    "\n",
    "# KORRIGIERTER COUNTPLOT F√úR DIE GR√ñSSE DER CLUSTER\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.countplot(\n",
    "    x='Cluster_ML_str', \n",
    "    data=df_final, \n",
    "    palette=COLOR_DISCRETE_MAP_SEABORN,\n",
    "    order=sorted(df_final['Cluster_ML_str'].unique()) \n",
    ")\n",
    "plt.title('Gr√∂√üe der Cluster')\n",
    "plt.xlabel('Cluster (ID)')\n",
    "plt.ylabel('Anzahl der Beobachtungen')\n",
    "plt.show()\n",
    "\n",
    "print('\\nFinale Datenform von df_ML (f√ºr Weiterverarbeitung):')\n",
    "print(df_ML[['loan_id', 'Cluster_ML']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520eca5d",
   "metadata": {},
   "source": [
    " - Begriff im Plot (Englisch/Code)\n",
    "    - Deutsche √úbersetzung / Erkl√§rung\n",
    " - dependents \n",
    "    - Anzahl der Abh√§ngigen (Kinder, andere Personen)\n",
    " - applicant_income \n",
    "    - Einkommen des Antragstellers (Hauptverdiener)\n",
    " - coapplicant_income \n",
    "    - Einkommen des Mitantragstellers (z.B. Partner)\n",
    " - loan_amount\t\n",
    "    - Kreditbetrag (H√∂he des Darlehens)\n",
    " - loan_amount_term \n",
    "    - Kreditlaufzeit (in der Regel in Monaten)\n",
    " - credit_history \n",
    "    - Kredit-Historie / Kreditw√ºrdigkeit (z.B. 1 f√ºr gut, 0 f√ºr schlecht)\n",
    " - total_income \n",
    "    - Gesamteinkommen (applicant_income + coapplicant_income)\n",
    " - loan_to_income_ratio \n",
    "    - Kredit-Einkommen-Verh√§ltnis (loan_amount / total_income)\n",
    " - loan_term_to_income_ratio \n",
    "    - Kreditlaufzeit\t-Einkommen-Verh√§ltnis (loan_amount_term / total_income)\n",
    " - gender_Male \n",
    "    - Geschlecht: M√§nnlich (One-Hot-Encoded Feature)\n",
    " - married_Yes \n",
    "    - Verheiratet: Ja (One-Hot-Encoded Feature)\n",
    " - education_Not_Graduate \n",
    "    - Ausbildung: Kein Hochschulabschluss (One-Hot-Encoded Feature)\n",
    " - self_employed_Yes \n",
    "    - Selbst√§ndig: Ja (One-Hot-Encoded Feature)\n",
    " - property_area_Semiurban \n",
    "    - Immobilienstandort: Halbst√§dtisch (Semiurban)\n",
    " - property_area_Urban \n",
    "    - Immobilienstandort: St√§dtisch (Urban)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22faa9",
   "metadata": {},
   "source": [
    "# Die Gesch√§ftsaufgabe \"Erstelle ein Modell zur Analyse von Darlehen\" ist erf√ºllt. \n",
    "# Funktionierendes Modell erstellt, das Vorhersagen √ºber die Kreditw√ºrdigkeit treffen kann.\n",
    "\n",
    "Der st√§rkste Aussagetr√§ger f√ºr diese Gesch√§ftsaufgabe ist die Feature-Importance-Analyse.\n",
    "\n",
    "Warum?\n",
    "\n",
    "- Erkl√§rt das \"Warum\": Ein reines Vorhersagemodell sagt nur ob ein Kredit wahrscheinlich ausf√§llt, aber nicht warum. Die Feature-Importance-Analyse zeigt uns genau, welche Faktoren die Entscheidung des Modells am st√§rksten beeinflussen. In unserem Fall ist das mit Abstand die credit_history.\n",
    "\n",
    "- Direkte Gesch√§ftsrelevanz: Diese Erkenntnis ist f√ºr das Gesch√§ft extrem wertvoll. Sie best√§tigt, dass die Kredithistorie eines Kunden der entscheidende Faktor ist. Das Management kann darauf basierend klarere und transparentere Kreditvergaberichtlinien erstellen.\n",
    "\n",
    "- Risikominimierung: Wenn die Bank wei√ü, dass die Kredithistorie der wichtigste Indikator ist, kann sie ihr Risiko besser steuern, indem sie bei Kunden mit schlechter oder fehlender Historie besonders vorsichtig ist oder h√∂here Zinsen ansetzt.\n",
    "\n",
    "- Datenqualit√§t: Die Analyse zeigt auch, wie wichtig die neu erstellten Merkmale wie loan_to_income_ratio sind. Das ist ein Hinweis f√ºr die Zukunft, dass es sich lohnt, in gutes Feature Engineering zu investieren.\n",
    "\n",
    "   # - Zusammenfassend:\n",
    "\n",
    "W√§hrend die Genauigkeit des Modells (z.B. 84% bei der Logistischen Regression) zeigt, dass das Modell funktioniert, erkl√§rt die Feature-Importance-Analyse, wie es funktioniert und liefert damit die tiefgreifendsten und handlungsorientiertesten Einblicke f√ºr das Gesch√§ft. Sie ist der Kern der \"Analyse\" in der Aufgabenstellung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15998020",
   "metadata": {},
   "source": [
    "## Zu verbessern \n",
    "welche Future Engeneering am beesten corelieren mit den Polar Plot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
