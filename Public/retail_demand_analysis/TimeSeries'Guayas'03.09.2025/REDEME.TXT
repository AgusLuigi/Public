Kursprojekt
ProjektÃ¼bersicht â€“ Was Sie im Laufe des 4-wÃ¶chigen Kurses bauen werden
Im kommenden Monat arbeiten Sie an einem durchgÃ¤ngigen Prognoseprojekt und fÃ¼gen jede Woche eine neue Ebene hinzu. Am Ende der Sitzung werden Sie Folgendes erstellt haben:
Explorative Datenanalyse (EDA)
Klare Visualisierungen und numerische Zusammenfassungen, die Trends, SaisonalitÃ¤t, Werbeaktionen, Feiertage und AusreiÃŸer im Favorita-Datensatz aufzeigen.
Datenaufbereitungspipeline
LÃ¼ckengefÃ¼llte Kalender, speziell entwickelte Kalenderfunktionen, VerzÃ¶gerungsvariablen und alle erforderlichen Transformationen â€“ bereit fÃ¼r die Modelleingabe.
Lagerartikelprognosen
Ein maschinelles Lernmodell, das die tÃ¤gliche Nachfrage nach jedem Produkt in jedem GeschÃ¤ft in der Provinz Guayas vorhersagt .
Zielprognosehorizont: Januar â€“ MÃ¤rz 2014 (einschlieÃŸlich).
Diese Woche verwenden wir den kompletten Datensatz; die zeitliche Aufteilung erfolgt spÃ¤ter beim Trainieren des Modells.
Leichtgewichtige Web-App
Ein einfaches Frontend (denken Sie an â€eine Seite + Endpunktâ€œ), Ã¼ber das die Bedarfsplaner von Guayas ein Produkt-Filial-Paar auswÃ¤hlen und Ihre Prognose abrufen kÃ¶nnen.
Live-Demo & Video-Rundgang
Sie prÃ¤sentieren die wichtigsten Ergebnisse, zeigen die App und teilen eine kurze Aufnahme zur ÃœberprÃ¼fung.
Die Notebooks der einzelnen Wochen bauen aufeinander auf. Achten Sie daher auf sauberen Code und fÃ¼hren Sie regelmÃ¤ÃŸig Commits durch. In Woche 4 verfÃ¼gen Sie Ã¼ber eine portfoliofÃ¤hige, vollstÃ¤ndig reproduzierbare LÃ¶sung zur Bedarfsprognose.
 

# Woche 1 â€“ Checkliste & Fahrplan
Diese Woche dreht sich alles darum, Ihren Arbeitsbereich einzurichten und die Rohdaten auf einen Ã¼berschaubaren Teil mit Fokus auf die Provinz Guayas zu reduzieren . Folgen Sie den unten stehenden Schritten und haken Sie diese nach und nach ab.
 
## Schalten Sie Ihr Arbeitsnotizbuch ein.
Erstelle (oder verwende) ein GitHub-Repository fÃ¼r das Kursprojekt. Benenne es beispielsweise â€Einzelhandelsnachfrageanalyseâ€œ. So findest du alle Notebooks, Skripte und den gesamten Commit-Verlauf an einem Ort.
Ã–ffnen Sie ein neues Google Colab -Notebook in Ihrem Drive-Ordner mit dem Namen data_prep.ipynb.
Speichern Sie eine erste Kopie auf GitHub ( File â†’ Save a copy in GitHub), damit das Repository seinen ersten Commit hat.
 
## Daten laden
    - Lesen Sie alle Support-CSVs ( items.csv, stores.csv, oil.csv, holidays.csv, transactions.csv).
    - FÃ¼r train.csvdie grÃ¶ÃŸte Datei streamen wir sie in BlÃ¶cken , wie wir es in unserer EDA-Vorlesung gesehen haben. Diesmal filtern wir sie, indem wir nur die GeschÃ¤fte in der "Guayas"Region einbeziehen.
    - FÃ¼r schnellere Experimente sollte die StichprobengrÃ¶ÃŸe reduziert werden. Um den Rechenaufwand zu verringern, werden zufÃ¤llig 300.000 Zeilen ausgewÃ¤hlt .
    - Behalten Sie nur die drei grÃ¶ÃŸten Produktfamilien bei (gemessen an der Anzahl der verschiedenen Artikel in jeder Familie).
        Durch die BeschrÃ¤nkung auf die wichtigsten Produktfamilien verringert sich die Anzahl der SKU-basierten Zeitreihen, die Sie diese Woche verarbeiten mÃ¼ssen.

Versuche es zunÃ¤chst selbst. Falls du nicht weiterkommst, Ã¶ffne den Tipp; falls das nicht ausreicht, wirf einen Blick in die LÃ¶sung.

- Hinweis
- LÃ¶sung
    - ÃœberprÃ¼fen Sie Ihren Fortschritt:
        - Speichern Sie die gefilterten Daten df_trainals Pickle (oder Parquet) auf Drive(ordnerablage), damit Sie sie neu laden kÃ¶nnen, ohne die Chunk-Schleife erneut ausfÃ¼hren zu mÃ¼ssen.
        - Pushe das Notebook erneut auf GitHub mit einer Commit-Nachricht wie â€Woche 1 Datenvorbereitung: Chunk laden, Guayas-Filter, Top-3-Familienâ€œ .
 
## Datenexploration & Merkmalsaufbau (folgen Sie der gleichen Vorgehensweise wie in den Vorlesungen, wenden Sie sie aber jetzt auf Guayas an) :
ğŸ”¥ Wichtiger Hinweis!
Wenn Google Colab wÃ¤hrend irgendeiner Phase dieses Projekts beim AusfÃ¼hren Ihres Codes abstÃ¼rzt, bedeutet dies, dass der Laufzeittyp geÃ¤ndert werden muss.

Schritt 1: Gehen Sie zur Registerkarte â€Laufzeitâ€œ und wÃ¤hlen Sie â€Laufzeittyp Ã¤ndernâ€œ .

Schritt 2: WÃ¤hlen Sie in einem Popup-Fenster entweder die eine v2-8 TPUoder die andere Option aus T4 GPUund klicken Sie dann auf â€Speichernâ€œ Ihrer Auswahl.

Schritt 3: FÃ¼hren Sie den zuvor abgestÃ¼rzten Code erneut aus. Er sollte nun funktionieren.

    - DatenqualitÃ¤tsprÃ¼fungen
        - Fehlende Werte: Nullwerte in jeder Spalte erkennen und behandeln.
        - Fehlende Kalendertage: Erstellen Sie fÃ¼r jedes (GeschÃ¤ft, Artikel) einen vollstÃ¤ndigen Tagesindex und fÃ¼llen Sie fehlende Tage mit unit_sales = 0.
        - AusreiÃŸer: Auf unmÃ¶gliche negative Werte oder extreme Spitzenwerte prÃ¼fen unit_sales; gegebenenfalls kÃ¼rzen, ersetzen oder kennzeichnen.
    - Feature-Entwicklung
        - Erstellen Sie die datumsbasierten Funktionen aus den Theorielektionen yearnach â€“ z. monthB. day_of_weekgleitende Mittelwerte usw.
        - FÃ¼ge alle zusÃ¤tzlichen Merkmale hinzu , die deiner Meinung nach fÃ¼r Guayas wichtig sein kÃ¶nnten (z. B. Aktionskennzeichnungen, verderbliche Waren, und erkunde andere Tische! Sei kreativ!).
    - Setzen Sie Ihre Arbeit fort
        - Exportieren Sie den bereinigten und hervorgehobenen DataFrame nach Drive guayas_prepared.csv(Sie werden ihn in Woche 2 wieder laden).
 
## EDA fÃ¼r die Region von Interesse ( "Guayas")
    - Ãœbertragen und erweitern Sie die visuellen Fragestellungen, die Sie fÃ¼r Pichincha beantwortet haben â€“ Trendlinien, Heatmaps zur SaisonalitÃ¤t, Auswirkungen von Feiertagen, Anteil verderblicher Waren â€“ und konzentrieren Sie sich nun ausschlieÃŸlich auf die Filialen in Guayas.
    - Gehen Sie der Sache auf den Grund, wenn Sie etwas Interessantes entdecken; tiefergehende Erkenntnisse zahlen sich spÃ¤ter in einer hÃ¶heren Modellgenauigkeit aus.
 
## Auf GitHub einchecken
    - Wenn Ihr Notebook fehlerfrei durchlÃ¤uft, wÃ¤hlen Sie Datei â†’ Speichern Sie eine Kopie in GitHub und laden Sie diese mit einer aussagekrÃ¤ftigen Commit-Nachricht hoch (z. B. â€Woche 1: Guayas EDA + Feature-Vorbereitungâ€œ).
 
â˜
Merke: Je sauberer und besser verstanden deine Daten sind, desto leistungsfÃ¤higer wird dein Modell sein. Betrachte diesen Erkundungsschritt als Grundlage fÃ¼r alles Folgende.

------------------------------------------------------------------------ nur fÃ¼r person nicht fÃ¼r KI gedacht der folgende Text
ğŸš€
Wichtig â€“ Die Vorlesungen sind nur Ihr Startpunkt.
Im Kurs haben wir die grundlegenden Schritte behandelt â€“ grundlegende Datenbereinigung, KalenderfÃ¼llungen und einige Feature-Ideen â€“, um Ihnen den Umgang mit Zeitreihendaten zu zeigen. Doch echte Erkenntnisse (und eine hÃ¶here Modellgenauigkeit) erhÃ¤lt man durch weiterfÃ¼hrende Analysen:
VerknÃ¼pfen Sie mehrere Tabellen: Versuchen Sie es beispielsweise mit Ã–lpreisen nach GeschÃ¤ftsregion, Transaktionszahlen oder Wetterdaten, falls Sie diese finden kÃ¶nnen.
Stellen Sie neue Fragen: Wirken sich Werbeaktionen unterschiedlich auf die Nachfrage nach verderblichen bzw. nicht verderblichen Waren aus? FÃ¼hrt die Woche vor dem Zahltag zu einem Nachfrageanstieg in bestimmten Produktkategorien?
Erfinden Sie individuelle Funktionen: Flaggen fÃ¼r FuÃŸballspieltage, kumulierte MonatsumsÃ¤tze oder einen ZÃ¤hler fÃ¼r die Tage seit dem letzten Warenausverkauf.
Betrachten Sie das Notizbuch als Spielwiese â€“ experimentieren Sie, verbessern Sie Ihre Vorgehensweise und dokumentieren Sie Ihre Erkenntnisse. Je mehr Aspekte Sie jetzt erkunden, desto solider (und besser zu verteidigen) wird Ihr Modell spÃ¤ter sein.
------------------------------------------------------------------------ ab hier wieder KI

Kursprojekt: Aufgaben fÃ¼r die zweite Woche
Diese Woche haben Sie verschiedene Methoden der Zeitreihenmodellierung kennengelernt. Jetzt ist es Zeit, sie in die Praxis umzusetzen!
Erinnern Sie sich, dass wir letzte Woche die Daten aus "Guayas"der Region vollstÃ¤ndig vorbereitet und untersucht haben? Diese Woche arbeiten wir weiter mit diesen vorverarbeiteten Daten, trainieren jetzt aber ein XGBoost-Modell, das die Nachfrage prognostiziert.

# Woche 2
Ziel: Setzen neuen Zeitreihenkenntnisse in die Praxis um, indem Sie ein XGBoost-Nachfrageprognosemodell (und optional ein LSTM) fÃ¼r den Datensatz der Region Guayas erstellen .

## Schritte
    - Laden Sie die Daten: Ã–ffnen Sie die vorverarbeitete CSV-/Parquet-Datei aus Woche 1. Vergewissern Sie sich, dass sie nur DatensÃ¤tze aus Guayas enthÃ¤lt .
    - Behalten Sie die Top-3-Elementfamilien: Aus der EDA der letzten Woche sollten Sie herausgefunden haben GROCERY I, dass BEVERAGESund CLEANINGdie drei Top-Familien sind. Filtern Sie den Datenrahmen, sodass nur diese drei Familien Ã¼brig bleiben .
    - Kalenderfenster begrenzen: FÃ¼r diesen Sprint modellieren wir nur den 1. Januar bis 31. MÃ¤rz 2014 .
        Hier ist der Hinweis, wie Sie es machen kÃ¶nnen:
        - max_date = '2014-04-01'  train = train[(train['date'] < max_date)]

    - Feature-Engineering
        - VerzÃ¶gerungen/Rollen und andere Interaktionsbegriffe, von denen Sie denken, dass sie XGBoost helfen kÃ¶nnten.
        - Optionale Bonusfunktionen:
            - Metadaten speichern : ZusammenfÃ¼hren df_storesamstore_nbr
            - Artikelmetadaten : ZusammenfÃ¼hren df_itemsamitem_nbr
    - Aufteilung Training/Test: Chronologische Aufteilung: z. B. Training im Januar und Februar , Test im MÃ¤rz . Kein zufÃ¤lliges Mischen!
    - Trennen Sie Features (Spaltennamen, die wir fÃ¼r Vorhersagen verwenden) und Target (das Ziel, das wir vorhersagen werden) von den Features. Tun Sie dies sowohl fÃ¼r Trainings- als auch fÃ¼r Testdatenteile.
    - Passen Sie den XGBoost-Regressor an
    - Auswerten und Visualisieren:
        - Vorhersagen fÃ¼r X_test, MAE/RMSE berechnen.
        - Diagramm y_testvs. y_predSchÃ¤tzen Sie Tage mit zu niedrigen/zu hohen Prognosen.
    - ZusÃ¤tzliche Herausforderung!
        - ErwÃ¤gen Sie den Aufbau und die Erprobung eines LSTM-Modells und vergleichen Sie es mit dem XGBoost-Modell.
    - Speichern Sie das Colab-Notizbuch in Ihrem GitHub-Repository
 
## Ergebnisse
    - Ein sauberes Colab-Notizbuch mit:
        - Datenvorbereitung, Feature Engineering, Modelltraining, Diagramme, Metriken.
        - (Optional) LSTM-Abschnitt.
Ãœbertragen Sie das Notebook in Ihr GitHub-Repository.


Kursprojekt: Aufgaben fÃ¼r die dritte Woche
Sie wissen jetzt, wie Sie nach den besten Hyperparametern suchen , Modelle bewerten und Experimente verfolgen . Lassen Sie uns dies mit dem Projekt CorporaciÃ³n Favorita in die Praxis umsetzen.

# Woche 3
Wir bleiben in Ihrem vorhandenen Colab-Notizbuch (wie im Projekt der letzten Woche) und fÃ¼hren die folgenden Schritte aus:
    - Bewerten Sie die XGBoost-Basislinie der letzten Woche mit realen Zahlen (wie MAE, RMSE, Bias, MAD, rMAD, MAPE).
    - Richten Sie MLflow ein . Richten Sie ein Experiment und einen Lauf ein, der die Ergebnisse des Modells speichert, das wir letzte Woche erstellt und oben ausgewertet haben. Protokollieren Sie einen ersten Lauf (Basis-XGB): Parameter, Metriken und ein Prognosediagramm.
    - Optimieren Sie XGBoost ( finden Sie den besten Satz an Hyperparametern), trainieren Sie erneut mit der besten Konfiguration, bewerten Sie erneut und protokollieren Sie einen zweiten Lauf .
    - (Optionaler Bonus) Machen Sie dasselbe fÃ¼r das LSTM-Modell und optimieren Sie es anschlieÃŸend. Protokollieren Sie die Ergebnisse fÃ¼r die Basislinie + optimiert.
    - Speichern Sie Ihre Vorverarbeitungsartefakte (Skalierer + Feature-Spalten) fÃ¼r nÃ¤chste Woche.
    - Versionieren Sie Ihre Arbeit : Ãœbertragen Sie das aktualisierte Notebook auf GitHub.
â˜ğŸ¼
Kurztipp fÃ¼r Sprint 3
Wenn Colab oder Jupyter sehr langsam lÃ¤uft oder sogar abstÃ¼rzt, liegt das normalerweise daran, dass Ihre Rastersuche zu viele Kombinationen enthÃ¤lt â€“ es kann ewig dauern, bis sie abgeschlossen ist!
Folgendes kÃ¶nnen Sie tun:
Seien Sie geduldig und lassen Sie es laufen (die perfekte Zeit, um ins Fitnessstudio zu gehen, Gitarre zu spielen oder etwas Lustiges zu tun).
Oder beschleunigen Sie die Arbeit, indem Sie bei Ihrer Rastersuche weniger Parameter verwenden, sodass weniger Kombinationen ausgefÃ¼hrt werden.

Hinweis: Zu Beginn der Lektionen haben wir jede Hyperparameter-Kombination in MLflow protokolliert, damit wir in der BenutzeroberflÃ¤che einen Gewinner auswÃ¤hlen konnten. FÃ¼r dieses Projekt ist das nicht erforderlich. Protokollieren Sie nur zwei LÃ¤ufe :
    - Basismodell (vor der Optimierung).
    - Bestes Modell nach der Optimierung (der Grid/RandomizedSearchCV-Gewinner).
Sie kÃ¶nnen den Code dieser Woche anpassen, indem Sie die MLflow-Protokollierung auÃŸerhalb der Suchschleife verschieben und nur die endgÃ¼ltig ausgewÃ¤hlte Konfiguration protokollieren.

Kursprojekt: Aufgaben fÃ¼r die vierte Woche

Diese Woche haben Sie zwei MÃ¶glichkeiten:
    - Erstelle eine kleine Streamlit-App
        - Zielgruppe: Bedarfsplaner in der Region Guayas .
        - Ziel: Prognosen mit dem besten Modell untersuchen, das Sie in Sprint 3 trainiert haben.
        - Anforderungen:
            - Prognosezeitraum Januarâ€“MÃ¤rz 2014 .
            - Stellen Sie die Prognosen in einem Ã¼bersichtlichen Diagramm dar.
            - MehrtÃ¤gige (N-tÃ¤gige) Vorhersagen zulassen.
    - Optimieren Sie Ihr Projekt fÃ¼r Ihr Portfolio
        - Konzentriere dich auf eine saubere Strukturierung deines Codes.
        - FÃ¼gen Sie eine klare Dokumentation hinzu.
        - Bereiten Sie Ihre PrÃ¤sentation vor.
        - Stelle sicher, dass es sich um ein gut organisiertes Portfolio-Projekt handelt, das du auf deinem GitHub-Konto prÃ¤sentieren kannst.
# Woche 4
## Option 1 Ziele Details â€“ Entwicklung und VerÃ¶ffentlichung der Streamlit-Vorhersage-App
    - Klonen Sie Ihr bestehendes retail_demand_forecastRepository (erstellt in Sprint 1) auf Ihren Computer und arbeiten Sie darin (dies geschieht mit Jupyter Notebook, nicht mit Google Colab).
    - Wechseln Sie auf Ihrem Computer in den   retail_demand_forecastentsprechenden Ordner (das ist das Repository, das Sie gerade geklont haben). Erstellen Sie die Streamlit-App-Struktur (der Jupyter-Dateibrowser ist ausreichend):
        - app/main.py(UI), app/config.py(Pfade/URIs/Konstanten),app/__init__.py
        - model/model_utils.py,model/__init__.py
        - data/data_utils.py,data/__init__.py
        - mlflow_results/(Lokaler MLflow-Speicher). Verwenden Sie Ihr bestes Modell aus Sprint 3 (den Lauf mit der besten Validierungsmetrik).
        - Erstellen Sie die requirements.txtDatei mit den Anforderungen fÃ¼r die Pakete.

README : ErlÃ¤uterung des Zwecks, der Modellwahl und der Leistung, der Konfiguration/AusfÃ¼hrung sowie HinzufÃ¼gen von Screenshots der lokalen Anwendung.
Den Code auf GitHub hochladen (keine groÃŸen Artefakte committen) und den Repo-Link einreichen.
Verwenden Sie den Link zum Repository auf GitHub, um Ihr Projekt zur ÃœberprÃ¼fung einzureichen.

â˜ğŸ¼
Schnelltipp fÃ¼r Sprint 4
Falls Jupyter beim Training der LSTM-Modelle extrem  langsam ist (was normal ist), kÃ¶nnen Sie es trotzdem abschlieÃŸen, wenn Sie warten. Wenn Sie es jedoch schneller haben mÃ¶chten:
    - Verwenden Sie weniger Parameter in Ihrer Gittersuche, oder
    - WÃ¤hlen Sie XGBoost fÃ¼r den Einsatz in Streamlit anstelle eines LSTM.

## Ergebnisse & Fokus der ÃœberprÃ¼fung
- Funktionierende App (lokal), die Vorhersagen fÃ¼r Januar bis MÃ¤rz 2014 fÃ¼r eine Guayas -Serie anzeigt.
- CodequalitÃ¤t : saubere Trennung in app/, data/, model/; keine schwerwiegenden Artefakte in Git.
- README : Zweck, Modellwahl (warum es das â€besteâ€œ ist), wichtige Kennzahlen aus Sprint 3, Einrichtungs-/AusfÃ¼hrungsanweisungen, Screenshots.
- NotizbÃ¼cher aus den Sprints 2â€“3: AufrÃ¤umen, Kommentare hinzufÃ¼gen; sie werden ebenfalls Ã¼berprÃ¼ft.

-----------
## Bonus: Streamlit-Selektoren
Idee: Nutzern ermÃ¶glichen, in Streamlit einen Shop und eine Artikelnummer fÃ¼r Vorhersagen auszuwÃ¤hlen .
Pythin code =
store = st.selectbox("Store", sorted({s for s,_ in SERIES.keys()}))
sku   = st.selectbox("Item (SKU)", sorted({i for s,i in SERIES.keys() if s == store}))
file_id = SERIES[(store, sku)]
-----------

Hinweis : Wenn die Zeit ablÃ¤uft

Nur wenn die Zeit ablÃ¤uft:
WÃ¤hlen Sie einen (oder mehrere) Shop(s) + eine (oder mehrere) Artikelnummer(n) (einzelne Serie), um die App schnell und zuverlÃ¤ssig zu halten.
MÃ¼ssen Ã¤ltere Notebooks aktualisiert werden?
Woche 2 (EDA):  Keine Ã„nderung erforderlich. Ihre EDA kann weiterhin breiter gefasst bleiben (Guayas + Top-Familien).
Woche 3 (Modellierung):  Ja, das Modell und die Daten mÃ¼ssen mit der von Ihnen verwendeten Datenreihe Ã¼bereinstimmen.
Wenn Ihr bestes Modell fÃ¼r alle Shops/SKUs oder fÃ¼r einen anderen Shop/eine andere SKU trainiert wurde , gilt Folgendes:
Trainiere/evaluiere das Training/die Auswertung fÃ¼r den ausgewÃ¤hlten Shop/die ausgewÃ¤hlte SKU und protokolliere diesen Lauf in MLflow, oder
Ã„ndern Sie die App so, dass sie die tatsÃ¤chlich trainierten Serien ausliefert .
Artefakte erneut exportieren (falls Sie neu trainieren): serialisierte Dateien , df_filteredund die App aktualisieren ( , ).scaler.pklfeature_cols.jsonconfig.pyMODEL_URIFILE_IDS["train"]
Minimal funktionsfÃ¤hige App: eine Guayas -Filiale â€“ SKU mit Prognosen fÃ¼r Januar bis MÃ¤rz 2014, unter Verwendung des am besten passenden Modells aus Woche 3.

Details zu Option 2
Ergebnisse & Fokus der ÃœberprÃ¼fung
- CodequalitÃ¤t : saubere Trennung in app/, data/, model/; keine schwerwiegenden Artefakte in Git.
- README : Zweck, Modellwahl (warum es das â€besteâ€œ ist), wichtige Kennzahlen aus Sprint 3, Einrichtungs-/AusfÃ¼hrungsanweisungen, Screenshots.
- NotizbÃ¼cher aus den Sprints 2â€“3: AufrÃ¤umen, Kommentare hinzufÃ¼gen; sie werden ebenfalls Ã¼berprÃ¼ft.
- Alles schÃ¶n Ã¼bersichtlich in GIT dargestellt.

## PrÃ¤sentationstag 
- Problemkontext (Bedarfsplanung fÃ¼r Guayas).
- Modellauswahl & Leistung (Kennzahlen aus Sprint 3; warum dieses Modell?).
- App-Ãœberblick: Datumsauswahl, N-Tage-Modus, Diagramm, CSV (falls diese Option gewÃ¤hlt wurde)
- Wie Planer es verwenden wÃ¼rden (Beispiel: Aktionswochen, Feiertage, Ã–lpreisschocks).
